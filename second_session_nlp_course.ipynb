{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vulmuLr-zXwh",
        "outputId": "a6779261-3891-494e-c01e-ed8267956ed3"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate datasets transformers accelerate sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtYyk9lvyJLe"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ejcv/NLP_course/blob/main/second_session_nlp_course.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# Introduction\n",
        "\n",
        "We have seen the traditional approach to NLP, but after the resurgence of neural networks, those become the state-of-the-art methods for NLP. In this session, we will see how to use neural networks for NLP tasks. We will start with the basics of neural networks and then move on to the advanced concepts. We will also see how to use pre-trained models for NLP tasks. We will use PyTorch for this session.\n",
        "\n",
        "In this session we will cover the following topics:\n",
        "\n",
        "- Introduction to Neural Networks\n",
        "- Activation Functions\n",
        "- Optimizers\n",
        "- Loss Functions\n",
        "- GRU and LSTM\n",
        "- Word Embeddings\n",
        "- Transformers (Attention mechanism)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoUMv6R1yJLg"
      },
      "source": [
        "## What is a neural network? üß†\n",
        "\n",
        "A neural network is a set of algorithms that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated. When created they were inspired by the way that biological nervous systems such as the brain process information, although today the architectures don't resemble biological neural networks.\n",
        "\n",
        "As the name implies, a neural network is a set of neurons which are connected to each other.\n",
        "<p align=\"center\">\n",
        "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/1200px-Neural_network_example.svg.png height=200>\n",
        "</p>\n",
        "\n",
        "## Perceptron üèãÔ∏è\n",
        "\n",
        "The perceptron is the building block of a neural network. It is a single neuron that takes in a set of inputs, performs some calculations, and outputs a value.\n",
        "<p align=\"center\">\n",
        "<img src=https://images.deepai.org/glossary-terms/perceptron-6168423.jpg height=200>\n",
        "</p>\n",
        "\n",
        "The perceptron true power comes from the activation function, which allows it to learn non-linear relationships between the inputs and outputs. Without it the perceptron is just a linear regression model.\n",
        "\n",
        "## Activation Functions üåã\n",
        "After we perform the weighted sum of the inputs and the weights, we need to pass the result through an activation function. The activation function changes the output of the perceptron based on the input. There are several different activation functions, depending on the problem we are trying to solve. Here are some of the most common ones:\n",
        "\n",
        "- Sigmoid\n",
        "- Tanh\n",
        "- ReLU\n",
        "- Leaky ReLU\n",
        "- Softmax\n",
        "\n",
        "You can find more details [here](https://en.wikipedia.org/wiki/Activation_function).\n",
        "\n",
        "\n",
        "# Loss functions üìâ\n",
        "\n",
        "The purpose of a neural network is to find the best set of weights that will minimize the loss function. The loss function is a measure of how far off the neural network is from the true values. The loss function is also known as the cost function or the error function. There are several different loss functions, depending on the problem we are trying to solve. Here are some of the most common ones:\n",
        "\n",
        "- Mean Squared Error\n",
        "- Mean Absolute Error\n",
        "- Binary Cross Entropy\n",
        "- Categorical Cross Entropy\n",
        "\n",
        "You can find more details [here](https://en.wikipedia.org/wiki/Loss_function).\n",
        "\n",
        "# Optimizers üöÄ\n",
        "\n",
        "The optimizer is the algorithm that will update the weights of the neural network. The optimizer is what makes the neural network learn. There are several different optimizers, depending on the problem we are trying to solve. Here are some of the most common ones:\n",
        "\n",
        "- Stochastic Gradient Descent\n",
        "- Adam\n",
        "- RMSProp\n",
        "- etc...\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=https://miro.medium.com/v2/resize:fit:1200/1*_osB82GKHBOT8k1idLqiqA.gif height=400>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQ6Rv4XyJLh"
      },
      "source": [
        "# Lets build a simple neural network to classify the tweet emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCvsWGD5yJLh",
        "outputId": "9b8fd031-bb53-41e7-b7ab-171628368764"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "451093"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will reuse some of the code from previous notebook, lets put it here\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# we will load the dataset\n",
        "df = pd.read_csv('datasets/tweet_emotions.csv')\n",
        "\n",
        "# We will change the function to replace specific usernames with the tag 'username'\n",
        "def replace_usernames(text: str) -> str:\n",
        "    \"\"\" A function that removes usernames from text. \"\"\"\n",
        "    return re.sub(r'@\\w+', 'username', text)\n",
        "\n",
        "# the tfidf vectorizer from sklearn has an option to use ngrams.... and do the preprocessing for us too :)\n",
        "\n",
        "# we will create an instance of the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(strip_accents='ascii', lowercase=True, stop_words='english', ngram_range=(1,3), preprocessor=lambda x: replace_usernames(x))\n",
        "\n",
        "# we will fit the vectorizer to the corpus\n",
        "# but now the corpus is the raw text in the dataframe\n",
        "vectorizer.fit(df['content'].tolist())\n",
        "\n",
        "# we will get the vocabulary size\n",
        "len(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RHVhUdkAyJLh",
        "outputId": "4fd18bec-d236-4ce8-e87e-f2517fb71488"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "      <th>sentiment_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968416</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     tweet_id   sentiment                                            content  \\\n",
              "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
              "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
              "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
              "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
              "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
              "\n",
              "   sentiment_id  \n",
              "0             0  \n",
              "1             1  \n",
              "2             1  \n",
              "3             2  \n",
              "4             3  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we enumerate the unique values and then convert it to a dictionary where the key is the index and the value is the sentiment\n",
        "id2sent = dict(enumerate(df['sentiment'].unique()))\n",
        "sent2id = {v:k for k,v in id2sent.items()}\n",
        "# now we can create a column with the numeric values\n",
        "df['sentiment_id'] = df['sentiment'].map(sent2id)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A7aXvzVyJLi",
        "outputId": "86d55406-5bd6-4f40-9240-5132866af329"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(40000, 451093)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get the tfidf matrix\n",
        "X = vectorizer.transform(df['content'].tolist())\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3IdA_jkyJLi",
        "outputId": "7222d182-75f1-4e5f-bb44-29b1f1661b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Check if there is any hardware acceleration available\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1QMAi0YyJLj",
        "outputId": "a2e4301e-f3ad-44a7-e7e2-1f5f9f0b4283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer_1): Linear(in_features=451093, out_features=10, bias=True)\n",
            "  (relu_1): ReLU()\n",
            "  (layer_2): Linear(in_features=10, out_features=13, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# lets use pytorch to build a simple neural network to classify the sentiment of the tweets\n",
        "\n",
        "\n",
        "INPUT_DIM=X.shape[1]\n",
        "OUTPUT_DIM=len(id2sent)\n",
        "\n",
        "model=torch.nn.Sequential()\n",
        "\n",
        "# add the first layer\n",
        "model.add_module(\"layer_1\",torch.nn.Linear(INPUT_DIM,10))\n",
        "model.add_module(\"relu_1\",torch.nn.ReLU())\n",
        "\n",
        "# add the second layer\n",
        "model.add_module(\"layer_2\",torch.nn.Linear(10,OUTPUT_DIM))\n",
        "\n",
        "\n",
        "# print the model\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvN7RWMyyJLj",
        "outputId": "cf4e0ad2-294e-4cf9-fa25-b7d26b15af3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = df['sentiment_id'].tolist()\n",
        "# one hot encode the labels\n",
        "y = torch.LongTensor(y)\n",
        "y = torch.nn.functional.one_hot(y).type(torch.FloatTensor)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yYbF7stcyJLk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = X.tocoo()\n",
        "X = torch.sparse.FloatTensor(torch.LongTensor([X.row.tolist(), X.col.tolist()]),\n",
        "                              torch.FloatTensor(X.data.astype(np.float32)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wZ7IH-JpyJLk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# we need to convert the data into tensors\n",
        "# we will use the torch.tensor method to convert the data into tensors\n",
        "# we will use the torch.utils.data.TensorDataset method to create a dataset\n",
        "# we will use the torch.utils.data.DataLoader method to create a dataloader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# lets create a dataset\n",
        "dataset = TensorDataset(X, y)\n",
        "\n",
        "total_count = len(dataset)\n",
        "train_count = int(0.8 * total_count)\n",
        "test_count = int(0.2 * total_count)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_count, test_count))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h4VUq9GDyJLk"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "from torch import nn\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# lets create a dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo-VhCOmyJLl",
        "outputId": "31389dab-6428-4a49-e9d8-458f5e842988"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.2020, -0.0149, -0.2882,  0.0921, -0.2487,  0.0795,  0.2833, -0.2781,\n",
              "         0.2117, -0.0040, -0.0005,  0.2893, -0.2049], grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(train_dataloader.dataset[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PifaCT1KyJLl"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to_dense()\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZTCIt51yJLl",
        "outputId": "46a518f2-ac4f-406f-bc96-74d8dbd582e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for t in range(epochs):\n",
        "  continue\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UBRfc0tyJLl"
      },
      "source": [
        "## Embeddings\n",
        "As we can see the performance is pretty bad, even though we are using neural networks. This is because we are not optimizing our representations. We are just using vectors with the size of the vocabulary. We can do better than that. We can use embeddings. Embeddings are a way to represent words in a vector space. The idea is that words that are similar will be close to each other in the vector space. We can use pre-trained embeddings or we can train our own embeddings.\n",
        "\n",
        "The ideal latent space (vector space) is where semantically similar words are close together. For example, the words \"car\" and \"automobile\" are similar, so they should be close together in the vector space. The same goes for \"bike\" and \"bicycle\". The words \"car\" and \"bicycle\" are not similar, so they should be far apart in the vector space.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:2000/1*SYiW1MUZul1NvL1kc1RxwQ.png\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "### The two most common ways to create word embeddings are:\n",
        "- **Word2Vec** (Mikolov et al., 2013)\n",
        "- **GloVe** (Pennington et al., 2014)\n",
        "\n",
        "\n",
        "## Should I train my own embedder model? ü§î\n",
        "It depends, most of the time is not worth it. If you have tons of data and enough computational power, then you can train your own model. Otherwise, you can use a pre-trained model. And if you have a very specific domain, you can do **fine-tuning**. Which means, taking advantage of the features the pre-trained representations have learned and then adjust the model to your domain.\n",
        "\n",
        "## Where do I get the pretrained models? ü§ó\n",
        "There are many places you can find the pretrained models, but the most popular one is [Hugging Face](https://huggingface.co/). They have a lot of models for different tasks and different languages. You can also find the models in the [TensorFlow Hub](https://tfhub.dev/).\n",
        "\n",
        "\n",
        "## Transformers ü§ñ\n",
        "Up to 2017, the word2vec and Glove approaches were the state-of-the-art, but that year a groundbreaking paper was published: [Attention is all you need](https://arxiv.org/abs/1706.03762). This paper introduced the **Transformer** architecture. This architecture is based on the attention mechanism. The attention mechanism is a way to learn the relationships between the words in a sentence. The transformer architecture is the base for the **BERT**, **GPT-{1:4}**, **Palm**, **LLaMA** models, which are the state-of-the-art model for NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1NijQzyJLl"
      },
      "source": [
        "# You can use a pretrained model in a downstream task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dlndNYy9yJLm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: This framework generates embeddings for each input sentence\n",
            "Embedding: [-1.37173980e-02 -4.28515561e-02 -1.56286508e-02  1.40537312e-02\n",
            "  3.95538285e-02  1.21796280e-01  2.94333734e-02 -3.17523777e-02\n",
            "  3.54959518e-02 -7.93140382e-02  1.75878499e-02 -4.04369496e-02\n",
            "  4.97259647e-02  2.54912600e-02 -7.18700662e-02  8.14968124e-02\n",
            "  1.47069187e-03  4.79627140e-02 -4.50335406e-02 -9.92174447e-02\n",
            " -2.81769410e-02  6.45045564e-02  4.44671251e-02 -4.76217046e-02\n",
            " -3.52952220e-02  4.38671559e-02 -5.28565943e-02  4.32974688e-04\n",
            "  1.01921484e-01  1.64072495e-02  3.26997079e-02 -3.45986448e-02\n",
            "  1.21339923e-02  7.94870779e-02  4.58342349e-03  1.57778580e-02\n",
            " -9.68202204e-03  2.87626311e-02 -5.05806915e-02 -1.55794229e-02\n",
            " -2.87907310e-02 -9.62286349e-03  3.15556489e-02  2.27349084e-02\n",
            "  8.71449336e-02 -3.85027863e-02 -8.84718671e-02 -8.75497423e-03\n",
            " -2.12343559e-02  2.08923556e-02 -9.02078226e-02 -5.25732487e-02\n",
            " -1.05638858e-02  2.88310666e-02 -1.61455218e-02  6.17834646e-03\n",
            " -1.23234186e-02 -1.07337208e-02  2.83354092e-02 -5.28567359e-02\n",
            " -3.58617716e-02 -5.97989187e-02 -1.09055229e-02  2.91567277e-02\n",
            "  7.97979385e-02 -3.27897520e-04  6.83501270e-03  1.32718347e-02\n",
            " -4.24619652e-02  1.87655557e-02 -9.89234746e-02  2.09049862e-02\n",
            " -8.69605616e-02 -1.50152314e-02 -4.86201458e-02  8.04414824e-02\n",
            " -3.67704988e-03 -6.65044188e-02  1.14556767e-01 -3.04229613e-02\n",
            "  2.96632126e-02 -2.80695129e-02  4.64989841e-02 -2.25513540e-02\n",
            "  8.54222849e-02  3.15446965e-02  7.34542012e-02 -2.21861675e-02\n",
            " -5.29678687e-02  1.27130197e-02 -5.27339727e-02 -1.06188729e-01\n",
            "  7.04731718e-02  2.76736841e-02 -8.05531591e-02  2.39649191e-02\n",
            " -2.65124943e-02 -2.17330884e-02  4.35275137e-02  4.84712273e-02\n",
            " -2.37066951e-02  2.85767745e-02  1.11846127e-01 -6.34935424e-02\n",
            " -1.58318356e-02 -2.26169564e-02 -1.31028499e-02 -1.62067835e-03\n",
            " -3.60929184e-02 -9.78297368e-02 -4.67728525e-02  1.76271871e-02\n",
            " -3.97492498e-02 -1.76399364e-04  3.39628011e-02 -2.09633913e-02\n",
            "  6.33663964e-03 -2.59410869e-02  8.10410902e-02  6.14393689e-02\n",
            " -5.44597162e-03  6.48275986e-02 -1.16844073e-01  2.36860830e-02\n",
            " -1.32058077e-02 -1.12476453e-01  1.90049261e-02 -1.74659232e-34\n",
            "  5.58950230e-02  1.94245037e-02  4.65439111e-02  5.18646166e-02\n",
            "  3.89390476e-02  3.40541117e-02 -4.32114191e-02  7.90637136e-02\n",
            " -9.79529694e-02 -1.27441455e-02 -2.91870926e-02  1.02052568e-02\n",
            "  1.88116468e-02  1.08942561e-01  6.63465261e-02 -5.35294861e-02\n",
            " -3.29228900e-02  4.69827875e-02  2.28882786e-02  2.74114199e-02\n",
            " -2.91983318e-02  3.12706120e-02 -2.22851112e-02 -1.02282166e-01\n",
            " -2.79116835e-02  1.13793444e-02  9.06308517e-02 -4.75414209e-02\n",
            " -1.00718990e-01 -1.23231839e-02 -7.96928480e-02 -1.44636547e-02\n",
            " -7.76401237e-02 -7.66921183e-03  9.73956287e-03  2.24205069e-02\n",
            "  7.77268782e-02 -3.17159994e-03  2.11538188e-02 -3.30394283e-02\n",
            "  9.55251604e-03 -3.73012349e-02  2.61360575e-02 -9.79085080e-03\n",
            " -6.31505847e-02  5.77435596e-03 -3.80031466e-02  1.29684471e-02\n",
            " -1.82499811e-02 -1.56282950e-02 -1.23366690e-03  5.55579513e-02\n",
            "  1.13105118e-04 -5.61256595e-02  7.40165487e-02  1.84452347e-02\n",
            " -2.66368389e-02  1.31951403e-02  7.50087276e-02 -2.46797334e-02\n",
            " -3.24006639e-02 -1.57674495e-02 -8.03524442e-03 -5.61324926e-03\n",
            "  1.05687128e-02  3.26162553e-03 -3.91989537e-02 -9.38677788e-02\n",
            "  1.14227168e-01  6.57304823e-02 -4.72634211e-02  1.45088043e-02\n",
            " -3.54490019e-02 -3.37761901e-02 -5.15505672e-02 -3.81002668e-03\n",
            " -5.15036248e-02 -5.93429320e-02 -1.69409264e-03  7.42107257e-02\n",
            " -4.20091599e-02 -7.19975382e-02  3.17249708e-02 -1.66303106e-02\n",
            "  3.96985933e-03 -6.52750358e-02  2.77391430e-02 -7.51650184e-02\n",
            "  2.27456111e-02 -3.91368382e-02  1.54315811e-02 -5.54908775e-02\n",
            "  1.23318294e-02 -2.59521157e-02  6.66423738e-02 -6.91261621e-34\n",
            "  3.31628807e-02  8.47929344e-02 -6.65584058e-02  3.33542004e-02\n",
            "  4.71609132e-03  1.35361487e-02 -5.38694039e-02  9.20694023e-02\n",
            " -2.96876319e-02  3.16220038e-02 -2.37497278e-02  1.98770780e-02\n",
            "  1.03446163e-01 -9.06947777e-02  6.30626874e-03  1.42886722e-02\n",
            "  1.19293993e-02  6.43725181e-03  4.20105197e-02  1.25343902e-02\n",
            "  3.93019356e-02  5.35691418e-02 -4.30749804e-02  6.10433072e-02\n",
            " -5.40001311e-05  6.91683069e-02  1.05520552e-02  1.22111682e-02\n",
            " -7.23184943e-02  2.50469837e-02 -5.18370941e-02 -4.36562635e-02\n",
            " -6.71818480e-02  1.34828389e-02 -7.25888833e-02  7.04166526e-03\n",
            "  6.58939555e-02  1.08994963e-02 -2.60011107e-03  5.49969450e-02\n",
            "  5.06966524e-02  3.27947661e-02 -6.68833330e-02  6.45557418e-02\n",
            " -2.52076220e-02 -2.92572770e-02 -1.16696641e-01  3.24065126e-02\n",
            "  5.85858934e-02 -3.51756550e-02 -7.15239868e-02  2.24935636e-02\n",
            " -1.00786634e-01 -4.74545397e-02 -7.61962757e-02 -5.87166548e-02\n",
            "  4.21137959e-02 -7.47213960e-02  1.98468119e-02 -3.36500071e-03\n",
            " -5.29736280e-02  2.74729561e-02  3.45736332e-02 -6.11846372e-02\n",
            "  1.06364802e-01 -9.64120030e-02 -4.55945358e-02  1.51488995e-02\n",
            " -5.13532385e-03 -6.64447248e-02  4.31721359e-02 -1.10406289e-02\n",
            " -9.80251003e-03  7.53782988e-02 -1.49570927e-02 -4.80208360e-02\n",
            "  5.80726676e-02 -2.43897010e-02 -2.23137178e-02 -4.36992794e-02\n",
            "  5.12053855e-02 -3.28625552e-02  1.08763367e-01  6.08926453e-02\n",
            "  3.30793275e-03  5.53820170e-02  8.43201652e-02  1.27087254e-02\n",
            "  3.84465568e-02  6.52325302e-02 -2.94683985e-02  5.08006029e-02\n",
            " -2.09347755e-02  1.46135673e-01  2.25562230e-02 -1.77227779e-08\n",
            " -5.02672978e-02 -2.79135420e-04 -1.00328624e-01  2.42810808e-02\n",
            " -7.54043311e-02 -3.79139893e-02  3.96049879e-02  3.10080163e-02\n",
            " -9.05697886e-03 -6.50411546e-02  4.05453257e-02  4.83390279e-02\n",
            " -4.56961952e-02  4.76005953e-03  2.64363387e-03  9.35614780e-02\n",
            " -4.02599573e-02  3.27402167e-02  1.18297637e-02  5.54344878e-02\n",
            "  1.48052275e-01  7.21189007e-02  2.77024228e-04  1.68650784e-02\n",
            "  8.34881328e-03 -8.76153447e-03 -1.33649623e-02  6.14237413e-02\n",
            "  1.57167967e-02  6.94960877e-02  1.08620934e-02  6.08018525e-02\n",
            " -5.33421226e-02 -3.47924158e-02 -3.36271934e-02  6.93906248e-02\n",
            "  1.22986836e-02 -1.45237342e-01 -2.06971564e-03 -4.61133160e-02\n",
            "  3.72739788e-03 -5.59355319e-03 -1.00659907e-01 -4.45953161e-02\n",
            "  5.40921241e-02  4.98894043e-03  1.49534978e-02 -8.26059207e-02\n",
            "  6.26629815e-02 -5.01906592e-03 -4.81858514e-02 -3.53991091e-02\n",
            "  9.03391372e-03 -2.42337398e-02  5.66267222e-02  2.51529235e-02\n",
            " -1.70709249e-02 -1.24780070e-02  3.19518521e-02  1.38421739e-02\n",
            " -1.55814914e-02  1.00178264e-01  1.23657212e-01 -4.22967076e-02]\n",
            "\n",
            "Sentence: Sentences are passed as a list of string.\n",
            "Embedding: [ 5.64524569e-02  5.50024211e-02  3.13795358e-02  3.39485034e-02\n",
            " -3.54247205e-02  8.34667832e-02  9.88800526e-02  7.27545517e-03\n",
            " -6.68657338e-03 -7.65813282e-03  7.93738291e-02  7.39680836e-04\n",
            "  1.49291372e-02 -1.51046766e-02  3.67674679e-02  4.78743464e-02\n",
            " -4.81969416e-02 -3.76052670e-02 -4.60277796e-02 -8.89816061e-02\n",
            "  1.20228164e-01  1.30663246e-01 -3.73936035e-02  2.47850013e-03\n",
            "  2.55823880e-03  7.25814700e-02 -6.80436492e-02 -5.24696372e-02\n",
            "  4.90234643e-02  2.99563594e-02 -5.84429838e-02 -2.02263035e-02\n",
            "  2.08821353e-02  9.76691917e-02  3.52390334e-02  3.91141176e-02\n",
            "  1.05668083e-02  1.56238198e-03 -1.30822537e-02  8.52906704e-03\n",
            " -4.84087318e-03 -2.03766525e-02 -2.71801092e-02  2.83308253e-02\n",
            "  3.66017073e-02  2.51276158e-02 -9.90861952e-02  1.15626715e-02\n",
            " -3.60381231e-02 -7.23784193e-02 -1.12670071e-01  1.12942411e-02\n",
            " -3.86397094e-02  4.67386246e-02 -2.88460217e-02  2.26703994e-02\n",
            " -8.52405187e-03  3.32814567e-02 -1.06579566e-03 -7.09744841e-02\n",
            " -6.31170347e-02 -5.72186820e-02 -6.16026409e-02  5.47146611e-02\n",
            "  1.18317557e-02 -4.66261096e-02  2.56959759e-02 -7.07415584e-03\n",
            " -5.73843196e-02  4.12839204e-02 -5.91503829e-02  5.89021482e-02\n",
            " -4.41697352e-02  4.65081595e-02 -3.15814726e-02  5.58312126e-02\n",
            "  5.54578826e-02 -5.96533902e-02  4.06407379e-02  4.83760564e-03\n",
            " -4.96768020e-02 -1.00944318e-01  3.40078138e-02  4.13275650e-03\n",
            " -2.93524843e-03  2.11838316e-02 -3.73962522e-02 -2.79066972e-02\n",
            " -4.61767502e-02  5.26139326e-02 -2.79735066e-02 -1.62379295e-01\n",
            "  6.61042705e-02  1.72274671e-02 -5.45114977e-03  4.74473722e-02\n",
            " -3.82237583e-02 -3.96896675e-02  1.34545416e-02  4.49653715e-02\n",
            "  4.53672884e-03  2.82978453e-02  8.36633220e-02 -1.00857634e-02\n",
            " -1.19354017e-01 -3.84624936e-02  4.82858568e-02 -9.46083888e-02\n",
            "  1.91854630e-02 -9.96518657e-02 -6.30596802e-02  3.02695986e-02\n",
            "  1.17402226e-02 -4.78373170e-02 -6.20274059e-03 -3.32850926e-02\n",
            " -4.04389482e-03  1.28307119e-02  4.05255072e-02  7.56476820e-02\n",
            "  2.92435195e-02  2.84270141e-02 -2.78938599e-02  1.66857690e-02\n",
            " -2.47961432e-02 -6.83651194e-02  2.89968774e-02 -5.39867674e-33\n",
            " -2.69009266e-03 -2.65069213e-02 -6.47943758e-04 -8.46199226e-03\n",
            " -7.35154450e-02  4.94084507e-03 -5.97842000e-02  1.03438050e-02\n",
            "  2.12902576e-03 -2.88213417e-03 -3.17076631e-02 -9.42364708e-02\n",
            "  3.03019956e-02  7.00227320e-02  4.50684763e-02  3.69439274e-02\n",
            "  1.13593591e-02  3.53026986e-02  5.50452620e-03  1.34416483e-03\n",
            "  3.46120656e-03  7.75048509e-02  5.45112751e-02 -7.92055875e-02\n",
            " -9.31696221e-02 -4.03398909e-02  3.10668591e-02 -3.83081362e-02\n",
            " -5.89442588e-02  1.93332136e-02 -2.67159957e-02 -7.91938752e-02\n",
            "  1.04204075e-04  7.70621225e-02  4.16603461e-02  8.90932456e-02\n",
            "  3.56842466e-02 -1.09153157e-02  3.71498987e-02 -2.07070317e-02\n",
            " -2.46101078e-02 -2.05025654e-02  2.62201801e-02  3.43590006e-02\n",
            "  4.39250395e-02 -8.20522010e-03 -8.40710327e-02  4.24171463e-02\n",
            "  4.87498641e-02  5.95384650e-02  2.87747644e-02  3.37639041e-02\n",
            " -4.07442264e-02 -1.66373781e-03  7.91928023e-02  3.41088623e-02\n",
            " -5.72839752e-04  1.87749378e-02 -1.36964619e-02  7.38333240e-02\n",
            "  5.74425037e-04  8.33505243e-02  5.60810752e-02 -1.13711301e-02\n",
            "  4.42611538e-02  2.69582383e-02 -4.80535887e-02 -3.15087698e-02\n",
            "  7.75226504e-02  1.81772765e-02 -8.83005112e-02 -7.85514899e-03\n",
            " -6.22243099e-02  7.19372034e-02 -2.33474988e-02  6.52482687e-03\n",
            " -9.49526019e-03 -9.88311991e-02  4.01306488e-02  3.07396837e-02\n",
            " -2.21607611e-02 -9.45911035e-02  1.02367904e-02  1.02187797e-01\n",
            " -4.12959754e-02 -3.15777436e-02  4.74751852e-02 -1.10209815e-01\n",
            "  1.69614777e-02 -3.71710137e-02 -1.03262113e-02 -4.72538956e-02\n",
            " -1.20215369e-02 -1.93255469e-02  5.79292625e-02  4.23867146e-34\n",
            "  3.92013229e-02  8.41360837e-02 -1.02946758e-01  6.92260042e-02\n",
            "  1.68821309e-02 -3.26761082e-02  9.65959020e-03  1.80900209e-02\n",
            "  2.17939820e-02  1.63188893e-02 -9.69292819e-02  3.74855171e-03\n",
            " -2.38456801e-02 -3.44055966e-02  7.11962730e-02  9.21950501e-04\n",
            " -6.23866450e-03  3.23754288e-02 -8.90361145e-04  5.01904823e-03\n",
            " -4.24537957e-02  9.89083499e-02 -4.60320972e-02  4.69704345e-02\n",
            " -1.75284036e-02 -7.02521857e-03  1.32743744e-02 -5.30152321e-02\n",
            "  2.66402424e-03  1.45819429e-02  7.43345032e-03 -3.07132006e-02\n",
            " -2.09416226e-02  8.24109837e-02 -5.15894815e-02 -2.71178484e-02\n",
            "  1.17583081e-01  7.72501668e-03 -1.89522691e-02  3.94559540e-02\n",
            "  7.17360973e-02  2.59117372e-02  2.75192056e-02  9.50545724e-03\n",
            " -3.02355252e-02 -4.07944098e-02 -1.04028486e-01 -7.97413290e-03\n",
            " -3.64457187e-03  3.29715982e-02 -2.35954840e-02 -7.50522455e-03\n",
            " -5.82233854e-02 -3.17905955e-02 -4.18049619e-02  2.17453390e-02\n",
            " -6.67292550e-02 -4.89104502e-02  4.58516786e-03 -2.66046282e-02\n",
            " -1.12596989e-01  5.11167645e-02  5.48534282e-02 -6.69856742e-02\n",
            "  1.26766309e-01 -8.59487206e-02 -5.94231077e-02 -2.92192050e-03\n",
            " -1.14876088e-02 -1.26025870e-01 -3.48279136e-03 -9.12002102e-02\n",
            " -1.22933134e-01  1.33777075e-02 -4.75775637e-02 -6.57933131e-02\n",
            " -3.39409448e-02 -3.07107791e-02 -5.22034317e-02 -2.35463586e-02\n",
            "  5.90035766e-02 -3.85758169e-02  3.19701135e-02  4.05118987e-02\n",
            "  1.67078301e-02 -3.58280763e-02  1.45687582e-02  3.20137963e-02\n",
            " -1.34843271e-02  6.07819892e-02 -8.31396692e-03 -1.08105214e-02\n",
            "  4.69410084e-02  7.66134858e-02 -4.23400290e-02 -2.11963282e-08\n",
            " -7.25292638e-02 -4.20228355e-02 -6.12374321e-02  5.24666049e-02\n",
            " -1.42363766e-02  1.18487524e-02 -1.40788713e-02 -3.67530286e-02\n",
            " -4.44977283e-02 -1.15140667e-02  5.23317382e-02  2.96652373e-02\n",
            " -4.62780446e-02 -3.70892994e-02  1.89129449e-02  2.04307269e-02\n",
            " -2.24005766e-02 -1.48562789e-02 -1.79504082e-02  4.20007892e-02\n",
            "  1.40942950e-02 -2.83492375e-02 -1.16863012e-01  1.48956627e-02\n",
            " -7.30564003e-04  5.66028766e-02 -2.68740468e-02  1.09106705e-01\n",
            "  2.94564059e-03  1.19267926e-01  1.14212409e-01  8.92974064e-02\n",
            " -1.70255620e-02 -4.99053895e-02 -2.11930927e-02  3.18421759e-02\n",
            "  7.03436062e-02 -1.02929451e-01  8.23817104e-02  2.81968173e-02\n",
            "  3.21146660e-02  3.79107930e-02 -1.09553136e-01  8.19620788e-02\n",
            "  8.73216316e-02 -5.73564097e-02 -2.01709531e-02 -5.69444075e-02\n",
            " -1.30338538e-02 -5.55684604e-02 -1.32966982e-02  8.64007697e-03\n",
            "  5.30011356e-02 -4.06846330e-02  2.71708816e-02 -2.55949073e-03\n",
            "  3.05775348e-02 -4.61865254e-02  4.68032248e-03 -3.64946723e-02\n",
            "  6.80802390e-02  6.65087327e-02  8.49152654e-02 -3.32849473e-02]\n",
            "\n",
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "Embedding: [ 4.39335145e-02  5.89343905e-02  4.81784418e-02  7.75481388e-02\n",
            "  2.67443974e-02 -3.76296453e-02 -2.60511879e-03 -5.99430390e-02\n",
            " -2.49604462e-03  2.20728256e-02  4.80259284e-02  5.57553135e-02\n",
            " -3.89453806e-02 -2.66167913e-02  7.69340573e-03 -2.62376592e-02\n",
            " -3.64160538e-02 -3.78161445e-02  7.40781128e-02 -4.95049655e-02\n",
            " -5.85217327e-02 -6.36197403e-02  3.24348919e-02  2.20085550e-02\n",
            " -7.10636899e-02 -3.31577398e-02 -6.94103912e-02 -5.00374027e-02\n",
            "  7.46267959e-02 -1.11133762e-01 -1.23063922e-02  3.77456173e-02\n",
            " -2.80313157e-02  1.45353554e-02 -3.15585509e-02 -8.05836692e-02\n",
            "  5.83526455e-02  2.59006093e-03  3.92802618e-02  2.57695615e-02\n",
            "  4.98505495e-02 -1.75619917e-03 -4.55297381e-02  2.92607993e-02\n",
            " -1.02017269e-01  5.22287227e-02 -7.90899917e-02 -1.02858180e-02\n",
            "  9.20244399e-03  1.30732497e-02 -4.04777341e-02 -2.77924985e-02\n",
            "  1.24667352e-02  6.72833845e-02  6.81247339e-02 -7.57124508e-03\n",
            " -6.09950349e-03 -4.23776619e-02  5.17815575e-02 -1.56707074e-02\n",
            "  9.56356991e-03  4.12390903e-02  2.14959793e-02  1.04293339e-02\n",
            "  2.73349229e-02  1.87062006e-02 -2.69607566e-02 -7.00541735e-02\n",
            " -1.04700461e-01 -1.89874298e-03  1.77016258e-02 -5.74725196e-02\n",
            " -1.44223068e-02  4.70444211e-04  2.33228435e-03 -2.51920484e-02\n",
            "  4.93003950e-02 -5.09610251e-02  6.31983653e-02  1.49164703e-02\n",
            " -2.70766933e-02 -4.52875048e-02 -4.90593761e-02  3.74940522e-02\n",
            "  3.84579338e-02  1.56901975e-03  3.09922509e-02  2.01630220e-02\n",
            " -1.24362772e-02 -3.06720063e-02 -2.78819352e-02 -6.89183101e-02\n",
            " -5.13677783e-02  2.14795973e-02  1.15746781e-02  1.25407637e-03\n",
            "  1.88765619e-02 -4.42318656e-02 -4.49816734e-02 -3.41867213e-03\n",
            "  1.31131187e-02  2.00100038e-02  1.21099807e-01  2.31075026e-02\n",
            " -2.20159627e-02 -3.28847207e-02 -3.15513229e-03  1.17846808e-04\n",
            "  9.91498455e-02  1.65239032e-02 -4.69672633e-03 -1.45366583e-02\n",
            " -3.71072395e-03  9.65135768e-02  2.85908151e-02  2.13481765e-02\n",
            " -7.17646107e-02 -2.41142046e-02 -4.40940782e-02 -1.07346907e-01\n",
            "  6.79945350e-02  1.30466774e-01 -7.97029436e-02  6.79513160e-03\n",
            " -2.37510968e-02 -4.61636633e-02 -2.99650654e-02 -3.69410047e-33\n",
            "  7.30969310e-02 -2.20171493e-02 -8.61464739e-02 -7.14379326e-02\n",
            " -6.36741370e-02 -7.21862763e-02 -5.93038648e-03 -2.33641304e-02\n",
            " -2.83658653e-02  4.77435030e-02 -8.06176662e-02 -1.56475813e-03\n",
            "  1.38444044e-02 -2.86236126e-02 -3.35387066e-02 -1.13777488e-01\n",
            " -9.17640235e-03 -1.08100856e-02  3.23195979e-02  5.88380769e-02\n",
            "  3.34208868e-02  1.07987933e-01 -3.72712649e-02 -2.96770148e-02\n",
            "  5.17190099e-02 -2.25339066e-02 -6.96091726e-02 -2.14475114e-02\n",
            " -2.33410429e-02  4.82199751e-02 -3.58766243e-02 -4.68990728e-02\n",
            " -3.97873037e-02  1.10813200e-01 -1.43007971e-02 -1.18464515e-01\n",
            "  5.82914837e-02 -6.25889376e-02 -2.94040777e-02  6.03238493e-02\n",
            " -2.44408892e-03  1.60116218e-02  2.67234053e-02  2.49530207e-02\n",
            " -6.49319291e-02 -1.06801540e-02  2.81464588e-02  1.03563871e-02\n",
            " -6.63591840e-04  1.98185574e-02 -3.04289162e-02  6.28422201e-03\n",
            "  5.15268706e-02 -4.75375466e-02 -6.44421503e-02  9.55031887e-02\n",
            "  7.55858123e-02 -2.81574782e-02 -3.49965990e-02  1.01816379e-01\n",
            "  1.98732354e-02 -3.68036442e-02  2.93523306e-03 -5.00746109e-02\n",
            "  1.50932118e-01 -6.16080314e-02 -8.58812779e-02  7.13999337e-03\n",
            " -1.33065525e-02  7.80405104e-02  1.75250154e-02  4.21279706e-02\n",
            "  3.57939042e-02 -1.32950380e-01  3.56970504e-02 -2.03117225e-02\n",
            "  1.24910465e-02 -3.80355045e-02  4.91543226e-02 -1.56541429e-02\n",
            "  1.21418223e-01 -8.08644667e-02 -4.68782149e-02  4.10843194e-02\n",
            " -1.84318237e-02  6.69690818e-02  4.33590449e-03  2.27315407e-02\n",
            " -1.36429016e-02 -4.53238599e-02 -3.92829850e-02 -6.29894016e-03\n",
            "  5.29609770e-02 -3.69064845e-02  7.11676553e-02  2.33343178e-33\n",
            "  1.05231389e-01 -4.81874309e-02  6.95919544e-02  6.56976104e-02\n",
            " -4.65149134e-02  5.14492206e-02 -1.24475155e-02  3.20872404e-02\n",
            " -9.23356712e-02  5.00933714e-02 -3.28876339e-02  1.39138736e-02\n",
            " -8.70262971e-04 -4.90905950e-03  1.03946343e-01  3.21621308e-04\n",
            "  5.28110079e-02 -1.17990281e-02  2.31565703e-02  1.31767523e-02\n",
            " -5.25962487e-02  3.26702110e-02  3.08673305e-04  6.41128868e-02\n",
            "  3.88500430e-02  5.88008277e-02  8.29793662e-02 -1.88150108e-02\n",
            " -2.26377472e-02 -1.00473650e-01 -3.83752771e-02 -5.88080809e-02\n",
            "  1.82421261e-03 -4.26995456e-02  2.50194930e-02  6.40060157e-02\n",
            " -3.77483070e-02 -6.83901506e-03 -2.54606456e-03 -9.76043120e-02\n",
            "  1.88475642e-02 -8.83208762e-04  1.73611827e-02  7.10790530e-02\n",
            "  3.30392942e-02  6.93418551e-03 -5.60522825e-02  5.14634401e-02\n",
            " -4.29542102e-02  4.60077189e-02 -8.78829416e-03  3.17289196e-02\n",
            "  4.93965223e-02  2.95189712e-02 -5.05192056e-02 -5.43186516e-02\n",
            "  1.49990155e-04 -2.76614707e-02  3.46878916e-02 -2.10889578e-02\n",
            "  1.38060376e-02  2.99886651e-02  1.39744654e-02 -4.26473608e-03\n",
            " -1.50337582e-02 -8.76095816e-02 -6.85054213e-02 -4.28141952e-02\n",
            "  7.76945427e-02 -7.10285380e-02 -7.37693533e-03  2.13727020e-02\n",
            "  1.35562634e-02 -7.90464878e-02  5.47665823e-03  8.30662772e-02\n",
            "  1.14148058e-01  1.80759770e-03  8.75490978e-02 -4.16045003e-02\n",
            "  1.55416597e-02 -1.01207132e-02 -7.32431747e-03  1.07966121e-02\n",
            " -6.62817135e-02  3.98414657e-02 -1.16711535e-01  6.42993972e-02\n",
            "  4.02919762e-02 -6.54741749e-02  1.95052363e-02  8.09996426e-02\n",
            "  5.36463261e-02  7.67969936e-02 -1.34852836e-02 -1.76919048e-08\n",
            " -4.43935059e-02  9.20642819e-03 -8.79590735e-02  4.26921919e-02\n",
            "  7.31366053e-02  1.68427546e-02 -4.03262973e-02  1.85131617e-02\n",
            "  8.44172239e-02 -3.74477655e-02  3.02996784e-02  2.90641580e-02\n",
            "  6.36878088e-02  2.89750323e-02 -1.47269936e-02  1.77542809e-02\n",
            " -3.36895250e-02  1.73161831e-02  3.37874927e-02  1.76826075e-01\n",
            " -1.75533462e-02 -6.03077821e-02 -1.43393828e-02 -2.38536429e-02\n",
            " -4.45530452e-02 -2.89850850e-02 -8.96776691e-02 -1.75934867e-03\n",
            " -2.61486061e-02  5.93997957e-03 -5.18355593e-02  8.57280046e-02\n",
            " -8.18398595e-02  8.35438631e-03  4.00790535e-02  4.17764746e-02\n",
            "  1.04573511e-01 -2.86570471e-03  1.96690746e-02  5.81049733e-03\n",
            "  1.33253569e-02  4.51000929e-02 -2.17587240e-02 -1.39492834e-02\n",
            " -6.86992407e-02 -2.94113741e-03 -3.10764872e-02 -1.05854407e-01\n",
            "  6.91624284e-02 -4.24114391e-02 -4.67682853e-02 -3.64751369e-02\n",
            "  4.50399518e-02  6.09816872e-02 -6.56561255e-02 -5.45639032e-03\n",
            " -1.86227262e-02 -6.31484464e-02 -3.87437120e-02  3.46734114e-02\n",
            "  5.55458404e-02  5.21627590e-02  5.61065376e-02  1.02063932e-01]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "#Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(40000, 384)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we can use that to embed our documents and then use a simple neural network to classify them.\n",
        "\n",
        "embeddings = model.encode(df['content'].apply(replace_usernames).tolist())\n",
        "embeddings.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer_1): Linear(in_features=384, out_features=10, bias=True)\n",
            "  (relu_1): ReLU()\n",
            "  (layer_2): Linear(in_features=10, out_features=13, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# lets define a new model\n",
        "\n",
        "INPUT_DIM=embeddings.shape[1]\n",
        "OUTPUT_DIM=len(id2sent)\n",
        "\n",
        "model=torch.nn.Sequential()\n",
        "\n",
        "# add the first layer\n",
        "model.add_module(\"layer_1\",torch.nn.Linear(INPUT_DIM,10))\n",
        "model.add_module(\"relu_1\",torch.nn.ReLU())\n",
        "\n",
        "# add the second layer\n",
        "model.add_module(\"layer_2\",torch.nn.Linear(10,OUTPUT_DIM))\n",
        "\n",
        "\n",
        "# print the model\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create the dataset\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "X = torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "y = df['sentiment_id'].tolist()\n",
        "# one hot encode the labels\n",
        "y = torch.LongTensor(y)\n",
        "y = torch.nn.functional.one_hot(y).type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "# lets create a dataset\n",
        "dataset = TensorDataset(X, y)\n",
        "\n",
        "total_count = len(dataset)\n",
        "train_count = int(0.8 * total_count)\n",
        "test_count = int(0.2 * total_count)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_count, test_count))\n",
        "\n",
        "# Initialize the loss function\n",
        "from torch import nn\n",
        "learning_rate = 1e-2\n",
        "batch_size = 256\n",
        "epochs = 300\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# lets create a dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.527625  [  256/32000]\n",
            "loss: 2.442829  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.8%, Avg loss: 2.435002 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.455846  [  256/32000]\n",
            "loss: 2.391146  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.1%, Avg loss: 2.364719 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.384548  [  256/32000]\n",
            "loss: 2.305785  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 21.9%, Avg loss: 2.310194 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.292166  [  256/32000]\n",
            "loss: 2.294126  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 21.8%, Avg loss: 2.264767 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.259405  [  256/32000]\n",
            "loss: 2.224792  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 21.8%, Avg loss: 2.233581 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.194399  [  256/32000]\n",
            "loss: 2.186820  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 21.8%, Avg loss: 2.207136 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.210243  [  256/32000]\n",
            "loss: 2.210610  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 21.8%, Avg loss: 2.193043 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.179055  [  256/32000]\n",
            "loss: 2.191583  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 21.8%, Avg loss: 2.182486 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.181810  [  256/32000]\n",
            "loss: 2.142729  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 21.9%, Avg loss: 2.175629 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.147594  [  256/32000]\n",
            "loss: 2.233354  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.1%, Avg loss: 2.170511 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.110198  [  256/32000]\n",
            "loss: 2.211783  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.2%, Avg loss: 2.164043 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.177958  [  256/32000]\n",
            "loss: 2.159832  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.4%, Avg loss: 2.161293 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.151478  [  256/32000]\n",
            "loss: 2.152837  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.4%, Avg loss: 2.162008 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.176851  [  256/32000]\n",
            "loss: 2.167550  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.4%, Avg loss: 2.153974 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.155712  [  256/32000]\n",
            "loss: 2.116596  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 22.6%, Avg loss: 2.155292 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.195564  [  256/32000]\n",
            "loss: 2.087879  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 23.7%, Avg loss: 2.155817 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.141756  [  256/32000]\n",
            "loss: 2.239476  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 23.8%, Avg loss: 2.149731 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.189767  [  256/32000]\n",
            "loss: 2.177359  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 23.3%, Avg loss: 2.149188 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.128030  [  256/32000]\n",
            "loss: 2.130019  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 24.4%, Avg loss: 2.148000 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.129802  [  256/32000]\n",
            "loss: 2.175915  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 23.7%, Avg loss: 2.147459 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 2.076338  [  256/32000]\n",
            "loss: 2.197411  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 23.9%, Avg loss: 2.149944 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 2.138065  [  256/32000]\n",
            "loss: 2.108058  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 24.5%, Avg loss: 2.145697 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 2.090207  [  256/32000]\n",
            "loss: 2.156000  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 24.4%, Avg loss: 2.146246 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 2.105753  [  256/32000]\n",
            "loss: 2.080877  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 25.1%, Avg loss: 2.139078 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 2.119172  [  256/32000]\n",
            "loss: 2.157000  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 25.4%, Avg loss: 2.137481 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 2.212231  [  256/32000]\n",
            "loss: 2.072538  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 25.2%, Avg loss: 2.143517 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 2.130507  [  256/32000]\n",
            "loss: 2.134246  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 25.6%, Avg loss: 2.140206 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 2.098886  [  256/32000]\n",
            "loss: 2.145272  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 25.4%, Avg loss: 2.137298 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 2.077643  [  256/32000]\n",
            "loss: 2.101002  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 25.7%, Avg loss: 2.140869 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 2.183602  [  256/32000]\n",
            "loss: 2.137839  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 25.9%, Avg loss: 2.137269 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 2.151362  [  256/32000]\n",
            "loss: 2.181974  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.2%, Avg loss: 2.132838 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 2.197952  [  256/32000]\n",
            "loss: 2.132062  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.3%, Avg loss: 2.132095 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 2.123809  [  256/32000]\n",
            "loss: 2.130907  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.4%, Avg loss: 2.135481 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 2.109805  [  256/32000]\n",
            "loss: 2.141625  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.4%, Avg loss: 2.135068 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 2.178112  [  256/32000]\n",
            "loss: 2.150856  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.6%, Avg loss: 2.131488 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 2.139406  [  256/32000]\n",
            "loss: 2.114229  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.5%, Avg loss: 2.130795 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 2.110432  [  256/32000]\n",
            "loss: 2.137012  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.7%, Avg loss: 2.135835 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 2.129387  [  256/32000]\n",
            "loss: 2.080781  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.8%, Avg loss: 2.129584 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 2.152906  [  256/32000]\n",
            "loss: 2.152787  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.8%, Avg loss: 2.128515 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 2.113855  [  256/32000]\n",
            "loss: 2.138821  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.8%, Avg loss: 2.129542 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 2.153129  [  256/32000]\n",
            "loss: 2.265134  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.0%, Avg loss: 2.125641 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 2.136237  [  256/32000]\n",
            "loss: 2.180066  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.0%, Avg loss: 2.127537 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 2.191097  [  256/32000]\n",
            "loss: 2.161469  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.9%, Avg loss: 2.122963 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 2.168738  [  256/32000]\n",
            "loss: 2.120562  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 26.9%, Avg loss: 2.117477 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 2.167587  [  256/32000]\n",
            "loss: 2.155371  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.0%, Avg loss: 2.120434 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 2.121908  [  256/32000]\n",
            "loss: 2.119575  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.117550 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 2.065443  [  256/32000]\n",
            "loss: 2.145494  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.117538 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 2.159463  [  256/32000]\n",
            "loss: 2.146973  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.115883 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 2.190549  [  256/32000]\n",
            "loss: 2.093658  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.1%, Avg loss: 2.117777 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 2.123318  [  256/32000]\n",
            "loss: 2.097399  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.111052 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 2.056953  [  256/32000]\n",
            "loss: 2.184593  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.109562 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 2.123244  [  256/32000]\n",
            "loss: 2.007588  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.108056 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 2.119202  [  256/32000]\n",
            "loss: 2.062591  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.109894 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 2.092876  [  256/32000]\n",
            "loss: 2.130664  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.4%, Avg loss: 2.107440 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 2.086689  [  256/32000]\n",
            "loss: 2.040848  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.6%, Avg loss: 2.104001 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 2.010876  [  256/32000]\n",
            "loss: 2.161390  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.6%, Avg loss: 2.097034 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 2.061287  [  256/32000]\n",
            "loss: 2.065653  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.9%, Avg loss: 2.096858 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 2.078057  [  256/32000]\n",
            "loss: 2.133985  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.9%, Avg loss: 2.097685 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 2.117162  [  256/32000]\n",
            "loss: 2.106880  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 27.9%, Avg loss: 2.093383 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 2.162366  [  256/32000]\n",
            "loss: 2.137522  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 28.0%, Avg loss: 2.093659 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 2.133307  [  256/32000]\n",
            "loss: 2.095275  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 28.2%, Avg loss: 2.090197 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 2.048890  [  256/32000]\n",
            "loss: 2.053718  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 28.4%, Avg loss: 2.085822 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 2.056701  [  256/32000]\n",
            "loss: 2.021803  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 28.5%, Avg loss: 2.086462 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 2.026948  [  256/32000]\n",
            "loss: 2.110144  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 28.8%, Avg loss: 2.085290 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 2.083935  [  256/32000]\n",
            "loss: 2.091043  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 29.2%, Avg loss: 2.079260 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 1.978511  [  256/32000]\n",
            "loss: 2.122217  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 29.0%, Avg loss: 2.083290 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 2.026591  [  256/32000]\n",
            "loss: 2.123616  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 29.3%, Avg loss: 2.081978 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 2.113277  [  256/32000]\n",
            "loss: 2.068022  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 29.4%, Avg loss: 2.071816 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 2.096156  [  256/32000]\n",
            "loss: 2.006263  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 29.5%, Avg loss: 2.070595 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 2.005182  [  256/32000]\n",
            "loss: 2.027548  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 29.8%, Avg loss: 2.068262 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 2.077633  [  256/32000]\n",
            "loss: 2.053052  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 29.9%, Avg loss: 2.064494 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 2.025355  [  256/32000]\n",
            "loss: 2.103281  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 2.062220 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 1.975630  [  256/32000]\n",
            "loss: 2.018276  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.2%, Avg loss: 2.055558 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 2.063669  [  256/32000]\n",
            "loss: 2.124146  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 2.055208 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 2.064947  [  256/32000]\n",
            "loss: 2.039072  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.2%, Avg loss: 2.052738 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 2.028901  [  256/32000]\n",
            "loss: 2.006352  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 2.054592 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 2.071792  [  256/32000]\n",
            "loss: 2.144613  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 2.051045 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 2.166844  [  256/32000]\n",
            "loss: 2.053357  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.6%, Avg loss: 2.043420 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 2.096216  [  256/32000]\n",
            "loss: 2.041229  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.5%, Avg loss: 2.036790 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 2.004252  [  256/32000]\n",
            "loss: 2.022237  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.6%, Avg loss: 2.041403 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 2.059975  [  256/32000]\n",
            "loss: 2.133414  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 2.032436 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 2.054217  [  256/32000]\n",
            "loss: 1.966230  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 30.8%, Avg loss: 2.033722 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 1.997885  [  256/32000]\n",
            "loss: 2.048505  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.1%, Avg loss: 2.027210 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 1.989864  [  256/32000]\n",
            "loss: 2.104232  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.1%, Avg loss: 2.029589 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 1.963017  [  256/32000]\n",
            "loss: 2.043686  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.2%, Avg loss: 2.020170 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 2.081405  [  256/32000]\n",
            "loss: 2.008927  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 2.019921 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 1.995892  [  256/32000]\n",
            "loss: 2.002527  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 2.021702 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 1.967053  [  256/32000]\n",
            "loss: 2.049644  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.4%, Avg loss: 2.021138 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 2.024900  [  256/32000]\n",
            "loss: 2.000651  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.4%, Avg loss: 2.016436 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 2.003733  [  256/32000]\n",
            "loss: 1.997264  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 2.010784 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 2.023676  [  256/32000]\n",
            "loss: 1.972141  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 2.004585 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 1.959642  [  256/32000]\n",
            "loss: 1.981112  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 31.8%, Avg loss: 2.007505 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 1.994134  [  256/32000]\n",
            "loss: 1.983575  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.005483 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 1.968041  [  256/32000]\n",
            "loss: 2.014633  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.001829 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 1.958774  [  256/32000]\n",
            "loss: 2.004104  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.002092 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 2.127545  [  256/32000]\n",
            "loss: 2.073174  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.997823 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 2.009526  [  256/32000]\n",
            "loss: 2.121365  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.996335 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 1.866095  [  256/32000]\n",
            "loss: 1.954380  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.992904 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 2.018755  [  256/32000]\n",
            "loss: 2.048419  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.2%, Avg loss: 1.986825 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 2.097040  [  256/32000]\n",
            "loss: 1.916200  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.986553 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "loss: 1.961958  [  256/32000]\n",
            "loss: 1.957115  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.987812 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "loss: 1.915418  [  256/32000]\n",
            "loss: 2.038246  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.981962 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "loss: 2.027826  [  256/32000]\n",
            "loss: 1.997943  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.1%, Avg loss: 1.986016 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "loss: 2.048352  [  256/32000]\n",
            "loss: 2.007873  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.2%, Avg loss: 1.980476 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "loss: 2.101482  [  256/32000]\n",
            "loss: 1.927802  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.2%, Avg loss: 1.976963 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "loss: 2.111457  [  256/32000]\n",
            "loss: 2.051347  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.974437 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "loss: 1.979144  [  256/32000]\n",
            "loss: 2.055397  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.970475 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "loss: 1.959665  [  256/32000]\n",
            "loss: 1.930792  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.973149 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "loss: 2.065318  [  256/32000]\n",
            "loss: 1.877629  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.2%, Avg loss: 1.972791 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "loss: 1.972309  [  256/32000]\n",
            "loss: 1.919076  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.4%, Avg loss: 1.968348 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "loss: 1.966262  [  256/32000]\n",
            "loss: 2.034368  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.964365 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "loss: 1.942424  [  256/32000]\n",
            "loss: 2.002449  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.4%, Avg loss: 1.962860 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "loss: 1.975204  [  256/32000]\n",
            "loss: 1.979342  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.4%, Avg loss: 1.963620 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "loss: 1.978089  [  256/32000]\n",
            "loss: 1.914734  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.4%, Avg loss: 1.959805 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "loss: 1.950500  [  256/32000]\n",
            "loss: 1.928174  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.5%, Avg loss: 1.965974 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "loss: 1.960111  [  256/32000]\n",
            "loss: 1.943225  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.957096 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "loss: 1.909566  [  256/32000]\n",
            "loss: 1.840165  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.6%, Avg loss: 1.957856 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "loss: 1.969474  [  256/32000]\n",
            "loss: 1.914776  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.956961 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "loss: 1.930412  [  256/32000]\n",
            "loss: 1.898355  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.8%, Avg loss: 1.955082 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "loss: 1.941946  [  256/32000]\n",
            "loss: 1.972279  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.8%, Avg loss: 1.954071 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "loss: 1.934711  [  256/32000]\n",
            "loss: 1.942832  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.9%, Avg loss: 1.953232 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "loss: 1.942454  [  256/32000]\n",
            "loss: 1.991252  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.9%, Avg loss: 1.947667 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "loss: 1.968052  [  256/32000]\n",
            "loss: 1.950656  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.9%, Avg loss: 1.952336 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "loss: 2.031868  [  256/32000]\n",
            "loss: 1.977344  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.946706 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "loss: 1.966998  [  256/32000]\n",
            "loss: 1.881686  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.945484 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "loss: 2.024423  [  256/32000]\n",
            "loss: 1.994508  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 32.9%, Avg loss: 1.945907 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "loss: 1.883915  [  256/32000]\n",
            "loss: 1.972376  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.948463 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "loss: 1.944981  [  256/32000]\n",
            "loss: 1.932237  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.943302 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "loss: 1.923059  [  256/32000]\n",
            "loss: 1.987038  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.1%, Avg loss: 1.946695 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "loss: 1.858218  [  256/32000]\n",
            "loss: 1.909370  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.944307 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "loss: 1.952286  [  256/32000]\n",
            "loss: 1.966495  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.1%, Avg loss: 1.938281 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "loss: 1.854983  [  256/32000]\n",
            "loss: 1.898920  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.939996 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "loss: 1.886434  [  256/32000]\n",
            "loss: 1.904522  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.1%, Avg loss: 1.936336 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "loss: 2.001563  [  256/32000]\n",
            "loss: 1.891215  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.1%, Avg loss: 1.940048 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "loss: 1.902051  [  256/32000]\n",
            "loss: 1.932151  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.2%, Avg loss: 1.936098 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "loss: 1.952791  [  256/32000]\n",
            "loss: 1.928152  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.2%, Avg loss: 1.938846 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "loss: 1.966842  [  256/32000]\n",
            "loss: 1.917015  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.934904 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "loss: 1.883212  [  256/32000]\n",
            "loss: 1.879416  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.4%, Avg loss: 1.935406 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "loss: 2.005039  [  256/32000]\n",
            "loss: 1.934486  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.4%, Avg loss: 1.931948 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "loss: 2.035228  [  256/32000]\n",
            "loss: 1.895180  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.935690 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "loss: 1.901727  [  256/32000]\n",
            "loss: 1.925519  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.929747 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "loss: 1.904020  [  256/32000]\n",
            "loss: 1.920222  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.4%, Avg loss: 1.934476 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "loss: 1.996629  [  256/32000]\n",
            "loss: 1.994345  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.4%, Avg loss: 1.928723 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "loss: 2.043948  [  256/32000]\n",
            "loss: 1.874262  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.5%, Avg loss: 1.930096 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "loss: 2.011096  [  256/32000]\n",
            "loss: 1.911798  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.5%, Avg loss: 1.927733 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "loss: 1.892043  [  256/32000]\n",
            "loss: 1.958374  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.5%, Avg loss: 1.928291 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "loss: 2.023574  [  256/32000]\n",
            "loss: 1.835119  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.5%, Avg loss: 1.929382 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "loss: 1.914418  [  256/32000]\n",
            "loss: 1.907360  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.6%, Avg loss: 1.929403 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "loss: 1.893313  [  256/32000]\n",
            "loss: 1.923268  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.6%, Avg loss: 1.924530 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "loss: 1.889492  [  256/32000]\n",
            "loss: 1.893417  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.927614 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "loss: 1.901758  [  256/32000]\n",
            "loss: 1.882851  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.8%, Avg loss: 1.930310 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "loss: 1.953993  [  256/32000]\n",
            "loss: 1.850524  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.8%, Avg loss: 1.929116 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "loss: 1.950360  [  256/32000]\n",
            "loss: 2.028733  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.8%, Avg loss: 1.920823 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "loss: 1.994623  [  256/32000]\n",
            "loss: 1.899456  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.8%, Avg loss: 1.922026 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "loss: 1.958565  [  256/32000]\n",
            "loss: 1.977124  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.926037 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "loss: 1.908865  [  256/32000]\n",
            "loss: 1.816411  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.8%, Avg loss: 1.920728 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "loss: 1.879573  [  256/32000]\n",
            "loss: 1.879567  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.924439 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "loss: 1.882612  [  256/32000]\n",
            "loss: 1.922162  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.921570 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "loss: 1.878336  [  256/32000]\n",
            "loss: 1.846548  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.922218 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "loss: 1.884134  [  256/32000]\n",
            "loss: 1.926152  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.918919 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "loss: 1.882195  [  256/32000]\n",
            "loss: 1.913724  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.918116 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "loss: 1.938843  [  256/32000]\n",
            "loss: 1.949069  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.915575 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "loss: 1.879643  [  256/32000]\n",
            "loss: 1.885565  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.910898 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "loss: 1.953561  [  256/32000]\n",
            "loss: 1.902187  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.1%, Avg loss: 1.918466 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "loss: 1.833033  [  256/32000]\n",
            "loss: 1.958683  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.1%, Avg loss: 1.918253 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "loss: 1.968334  [  256/32000]\n",
            "loss: 1.922562  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.2%, Avg loss: 1.915405 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "loss: 1.934023  [  256/32000]\n",
            "loss: 1.911086  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.1%, Avg loss: 1.914213 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "loss: 1.950317  [  256/32000]\n",
            "loss: 1.993025  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.913054 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "loss: 1.951431  [  256/32000]\n",
            "loss: 1.952171  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.914114 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "loss: 1.869786  [  256/32000]\n",
            "loss: 2.031654  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.914964 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "loss: 1.816304  [  256/32000]\n",
            "loss: 1.892486  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.911108 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "loss: 1.939745  [  256/32000]\n",
            "loss: 1.890117  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.914724 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "loss: 1.881410  [  256/32000]\n",
            "loss: 1.827927  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.911981 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "loss: 1.890142  [  256/32000]\n",
            "loss: 1.905378  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.916042 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "loss: 1.985127  [  256/32000]\n",
            "loss: 1.890608  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.913489 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "loss: 1.856216  [  256/32000]\n",
            "loss: 1.880896  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.911969 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "loss: 1.807033  [  256/32000]\n",
            "loss: 1.957881  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.912760 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "loss: 1.847411  [  256/32000]\n",
            "loss: 1.924390  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.918215 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "loss: 1.845959  [  256/32000]\n",
            "loss: 1.766020  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.908383 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "loss: 1.810451  [  256/32000]\n",
            "loss: 1.942451  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.910715 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "loss: 1.829872  [  256/32000]\n",
            "loss: 1.855142  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.909112 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "loss: 1.993595  [  256/32000]\n",
            "loss: 1.837592  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.905147 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "loss: 1.906552  [  256/32000]\n",
            "loss: 1.891561  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.910791 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "loss: 1.820605  [  256/32000]\n",
            "loss: 1.924105  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.909650 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "loss: 1.824288  [  256/32000]\n",
            "loss: 1.893497  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.910278 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "loss: 1.867927  [  256/32000]\n",
            "loss: 1.982507  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.905545 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "loss: 1.902507  [  256/32000]\n",
            "loss: 1.947413  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.913233 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "loss: 1.899279  [  256/32000]\n",
            "loss: 1.939330  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.907699 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "loss: 1.890097  [  256/32000]\n",
            "loss: 1.856566  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.909940 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "loss: 1.910717  [  256/32000]\n",
            "loss: 1.790315  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.910823 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "loss: 1.785199  [  256/32000]\n",
            "loss: 1.980861  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.902873 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "loss: 1.914599  [  256/32000]\n",
            "loss: 1.832562  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.907525 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "loss: 1.843460  [  256/32000]\n",
            "loss: 1.832292  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.907653 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "loss: 1.855619  [  256/32000]\n",
            "loss: 1.830926  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.909070 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "loss: 1.804419  [  256/32000]\n",
            "loss: 1.835021  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.903341 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "loss: 1.916195  [  256/32000]\n",
            "loss: 1.901477  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.902899 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "loss: 1.888967  [  256/32000]\n",
            "loss: 1.828029  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.908212 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "loss: 1.916874  [  256/32000]\n",
            "loss: 1.831340  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.903722 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "loss: 1.974906  [  256/32000]\n",
            "loss: 1.978869  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.907529 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "loss: 1.768093  [  256/32000]\n",
            "loss: 1.996062  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.906179 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "loss: 1.878050  [  256/32000]\n",
            "loss: 1.903533  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.898943 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "loss: 1.886047  [  256/32000]\n",
            "loss: 1.780174  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 1.902980 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "loss: 1.835394  [  256/32000]\n",
            "loss: 1.955398  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.900506 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "loss: 1.926871  [  256/32000]\n",
            "loss: 1.845321  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.905228 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "loss: 1.905318  [  256/32000]\n",
            "loss: 1.912710  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.902304 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "loss: 1.852605  [  256/32000]\n",
            "loss: 1.884272  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.900267 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "loss: 1.919363  [  256/32000]\n",
            "loss: 1.887423  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.905315 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "loss: 1.870052  [  256/32000]\n",
            "loss: 1.871126  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.901347 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "loss: 1.883312  [  256/32000]\n",
            "loss: 1.906052  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.906434 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "loss: 1.846393  [  256/32000]\n",
            "loss: 1.890952  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.899744 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "loss: 2.030142  [  256/32000]\n",
            "loss: 1.906458  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.897894 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "loss: 1.858581  [  256/32000]\n",
            "loss: 1.887503  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.898960 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "loss: 1.877298  [  256/32000]\n",
            "loss: 1.889073  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.897036 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "loss: 1.976170  [  256/32000]\n",
            "loss: 1.801974  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.900487 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "loss: 1.813681  [  256/32000]\n",
            "loss: 1.842747  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.897189 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "loss: 1.883740  [  256/32000]\n",
            "loss: 1.848491  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.900597 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "loss: 1.873351  [  256/32000]\n",
            "loss: 1.898903  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.903874 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "loss: 1.871300  [  256/32000]\n",
            "loss: 1.948612  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.898781 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "loss: 1.851299  [  256/32000]\n",
            "loss: 1.873079  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.903914 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "loss: 1.893108  [  256/32000]\n",
            "loss: 1.844441  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.891843 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "loss: 1.890429  [  256/32000]\n",
            "loss: 1.820735  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.896947 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "loss: 1.840147  [  256/32000]\n",
            "loss: 1.916271  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.891409 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "loss: 1.868489  [  256/32000]\n",
            "loss: 1.871966  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.898544 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "loss: 1.775277  [  256/32000]\n",
            "loss: 1.820048  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.899037 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "loss: 1.892018  [  256/32000]\n",
            "loss: 1.928488  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.897669 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "loss: 1.844151  [  256/32000]\n",
            "loss: 1.818064  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.897738 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "loss: 1.908611  [  256/32000]\n",
            "loss: 1.958210  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.897617 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "loss: 1.957608  [  256/32000]\n",
            "loss: 1.987148  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.897410 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "loss: 1.871952  [  256/32000]\n",
            "loss: 1.755541  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.899865 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "loss: 1.809579  [  256/32000]\n",
            "loss: 1.908736  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.893137 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "loss: 1.921604  [  256/32000]\n",
            "loss: 1.902679  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.896180 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "loss: 1.837064  [  256/32000]\n",
            "loss: 1.893808  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.899068 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "loss: 1.978112  [  256/32000]\n",
            "loss: 1.883874  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.890073 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "loss: 1.953553  [  256/32000]\n",
            "loss: 1.908215  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.898176 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "loss: 1.865482  [  256/32000]\n",
            "loss: 1.924629  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.891991 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "loss: 1.902810  [  256/32000]\n",
            "loss: 1.860849  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.896628 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "loss: 1.914204  [  256/32000]\n",
            "loss: 1.922804  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.892495 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "loss: 1.809106  [  256/32000]\n",
            "loss: 1.815015  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.894536 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "loss: 1.773725  [  256/32000]\n",
            "loss: 1.806146  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.897738 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "loss: 1.901975  [  256/32000]\n",
            "loss: 1.842187  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.884976 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "loss: 1.845603  [  256/32000]\n",
            "loss: 1.898237  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.894428 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "loss: 1.954549  [  256/32000]\n",
            "loss: 1.860653  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.895250 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "loss: 1.802079  [  256/32000]\n",
            "loss: 1.781292  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.890404 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "loss: 1.913294  [  256/32000]\n",
            "loss: 1.958946  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.893575 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "loss: 1.864120  [  256/32000]\n",
            "loss: 1.872637  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.891707 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "loss: 1.946786  [  256/32000]\n",
            "loss: 1.817449  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.890318 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "loss: 1.925423  [  256/32000]\n",
            "loss: 1.942572  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.890098 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "loss: 1.901603  [  256/32000]\n",
            "loss: 1.838838  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.894539 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "loss: 1.925603  [  256/32000]\n",
            "loss: 1.819643  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.890986 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "loss: 1.909580  [  256/32000]\n",
            "loss: 1.881273  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.889047 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "loss: 1.917010  [  256/32000]\n",
            "loss: 1.910984  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.895314 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "loss: 1.844876  [  256/32000]\n",
            "loss: 1.904186  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.893350 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "loss: 1.848148  [  256/32000]\n",
            "loss: 1.808191  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.889298 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "loss: 1.888737  [  256/32000]\n",
            "loss: 1.857937  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.888866 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "loss: 1.873697  [  256/32000]\n",
            "loss: 1.820411  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.888366 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "loss: 1.903508  [  256/32000]\n",
            "loss: 1.913521  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.883514 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "loss: 1.867780  [  256/32000]\n",
            "loss: 1.814617  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.889854 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "loss: 1.841204  [  256/32000]\n",
            "loss: 1.918377  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.888565 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "loss: 1.778467  [  256/32000]\n",
            "loss: 1.862727  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.890214 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "loss: 1.854425  [  256/32000]\n",
            "loss: 1.941131  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.889628 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "loss: 1.822811  [  256/32000]\n",
            "loss: 1.792824  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.887165 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "loss: 1.856215  [  256/32000]\n",
            "loss: 1.774003  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.889608 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "loss: 1.806138  [  256/32000]\n",
            "loss: 1.768972  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.882112 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "loss: 1.826088  [  256/32000]\n",
            "loss: 1.963341  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.885001 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "loss: 1.855202  [  256/32000]\n",
            "loss: 1.902467  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.889562 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "loss: 1.843570  [  256/32000]\n",
            "loss: 1.843084  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.889617 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "loss: 1.771003  [  256/32000]\n",
            "loss: 1.971433  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.886511 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "loss: 1.908754  [  256/32000]\n",
            "loss: 1.744472  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.885832 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "loss: 1.880910  [  256/32000]\n",
            "loss: 1.914500  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.887198 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "loss: 1.856568  [  256/32000]\n",
            "loss: 1.846193  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.883546 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "loss: 1.853592  [  256/32000]\n",
            "loss: 1.710378  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.886895 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "loss: 1.915055  [  256/32000]\n",
            "loss: 2.012527  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.886482 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "loss: 1.803344  [  256/32000]\n",
            "loss: 1.969643  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.889174 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "loss: 1.927612  [  256/32000]\n",
            "loss: 1.817086  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.886212 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "loss: 1.800566  [  256/32000]\n",
            "loss: 1.836394  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.885049 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "loss: 1.880894  [  256/32000]\n",
            "loss: 1.886682  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.889925 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "loss: 1.948658  [  256/32000]\n",
            "loss: 1.942669  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.886210 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "loss: 1.820934  [  256/32000]\n",
            "loss: 1.866736  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.883219 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "loss: 1.786899  [  256/32000]\n",
            "loss: 1.879642  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.883372 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "loss: 1.887357  [  256/32000]\n",
            "loss: 2.009705  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.884284 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "loss: 1.905446  [  256/32000]\n",
            "loss: 1.820506  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.884404 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "loss: 1.904685  [  256/32000]\n",
            "loss: 1.928917  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.887024 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "loss: 1.872363  [  256/32000]\n",
            "loss: 1.724167  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.887000 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "loss: 1.844825  [  256/32000]\n",
            "loss: 1.891448  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.885246 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "loss: 1.808169  [  256/32000]\n",
            "loss: 1.889857  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.881726 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "loss: 1.951747  [  256/32000]\n",
            "loss: 1.909750  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.886517 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "loss: 1.813118  [  256/32000]\n",
            "loss: 1.831750  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.882926 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "loss: 1.715018  [  256/32000]\n",
            "loss: 1.840374  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.886313 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "loss: 1.891610  [  256/32000]\n",
            "loss: 1.846927  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.882545 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "loss: 1.887367  [  256/32000]\n",
            "loss: 1.927617  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.878014 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "loss: 1.893560  [  256/32000]\n",
            "loss: 1.904720  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.877715 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "loss: 1.830810  [  256/32000]\n",
            "loss: 1.720568  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.878570 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "loss: 1.732863  [  256/32000]\n",
            "loss: 1.758945  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.878881 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "loss: 1.756536  [  256/32000]\n",
            "loss: 1.912822  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.887411 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "loss: 1.895253  [  256/32000]\n",
            "loss: 1.883398  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.882733 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "loss: 1.854006  [  256/32000]\n",
            "loss: 1.912948  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.890221 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "loss: 1.824985  [  256/32000]\n",
            "loss: 1.763587  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.879065 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "loss: 1.883921  [  256/32000]\n",
            "loss: 1.895322  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.887333 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "loss: 1.838813  [  256/32000]\n",
            "loss: 1.856479  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 1.879200 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "loss: 1.816978  [  256/32000]\n",
            "loss: 1.839007  [25856/32000]\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.883554 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLy0KFsyJLn"
      },
      "source": [
        "# Fine tune a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGjpEc8EyJLn",
        "outputId": "361b6227-da36-48aa-d92a-7986975d0f3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 36000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "df['label'] = df['sentiment_id']\n",
        "df['text'] = df['content']\n",
        "\n",
        "dataset = Dataset.from_pandas(df[['text', 'label']])\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "42780ac5ef1149409aa78d2d8f1dc75d",
            "2d35ea464b29419c9be540374cf6b0fa",
            "dcd3b14e89d444768e3bc5340c280f1c",
            "f5c188cc8c0649af84ce15d9771d6161",
            "d55e313cdca7482fb67be4be6e6045a8",
            "1a1708992fd54cf7ab63e80eec3304c3",
            "777035fbf0ba47b68ba142200138bb64",
            "ae1998e47e6b4f5284b7037827841f6d",
            "4f2c5396287746cc858ee7a10aa0cb1f",
            "4c1a7c24bdc14387a966b58c4f6ea912",
            "49c5424a787240f6a0c540d90e95b17b",
            "b0367179756641e3a80e3a06c2821ae2",
            "4c19b7dc46b240c8be1b77f270bb06d7",
            "77016231069a4551a958125c5ec1e3b5",
            "c3869650664d45279de9b88216ea49fa",
            "77d5f91908194a3184cfeb16e6fe7d55",
            "859ff521bd40497a86d5f85eca49f697",
            "50c8d08ce2c6445f83c3b3daaab4a900",
            "462e7a0b54024facb7103fa036854e2c",
            "0a10ed4362fb46b08024ca1e141a6553",
            "c0597b2d4aa74ce6b7c07d5980236758",
            "49114b5ed461432ba67c371b33e30e7c"
          ]
        },
        "id": "9_9IACZXyJLn",
        "outputId": "f8692af4-2fb0-4cf6-b958-fc5e81efef38"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42780ac5ef1149409aa78d2d8f1dc75d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0367179756641e3a80e3a06c2821ae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6NpB9MHzwqd",
        "outputId": "fb0750bb-a87d-4610-fb47-f35a7912df27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '@LifewithMikey82 this is the end of the semester so i am really busy... have got no time for twitting  what a pity cause i love it',\n",
              " 'label': 6,\n",
              " 'input_ids': [101,\n",
              "  137,\n",
              "  2583,\n",
              "  22922,\n",
              "  2107,\n",
              "  13012,\n",
              "  1183,\n",
              "  1604,\n",
              "  1477,\n",
              "  1142,\n",
              "  1110,\n",
              "  1103,\n",
              "  1322,\n",
              "  1104,\n",
              "  1103,\n",
              "  14594,\n",
              "  1177,\n",
              "  178,\n",
              "  1821,\n",
              "  1541,\n",
              "  5116,\n",
              "  119,\n",
              "  119,\n",
              "  119,\n",
              "  1138,\n",
              "  1400,\n",
              "  1185,\n",
              "  1159,\n",
              "  1111,\n",
              "  189,\n",
              "  10073,\n",
              "  19162,\n",
              "  1184,\n",
              "  170,\n",
              "  13532,\n",
              "  2612,\n",
              "  178,\n",
              "  1567,\n",
              "  1122,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'token_type_ids': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets['train'].select(range(1))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0Elr4xUkyJLn"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFdIEhCjyJLn",
        "outputId": "fdb83f5a-a1e9-4f35-89ca-e9ab1b1f7f5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=OUTPUT_DIM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AY0UGmQ9yJLo"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "09e3df7040ff4bd48bf0ee990fb61e03",
            "9a305842d0bd4900afd6310c9cc3c8d1",
            "2d3bad8aa71c4def85cce93a3e7c7d19",
            "88e9212df0684ca38ea5f990b04ecbb6",
            "24591885b81d4e289d089f3049d5a886",
            "c97bd8f0d4a44467911944af98379f7b",
            "a6b8e39d47b34667b1d1096ea0c1a720",
            "0908fc508cc84692b403d5eefb31b86a",
            "1bb40c60d89d4b55b4f51e23c15eaf95",
            "2355e46b3df445e3be8280a842b25ae5",
            "4cd4d6e504bc4ad5808c5ec3e041444f"
          ]
        },
        "id": "4Jp_ltW-yJLo",
        "outputId": "7173c9da-9bf0-4c35-f261-12c498acbcf6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09e3df7040ff4bd48bf0ee990fb61e03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ImuAuWk6yJLo"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s60Ek-vAyJLo"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IPCmU4WIyJLo"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "KztP6VHhyJLo",
        "outputId": "730474e6-76fa-41f1-c934-088055617fec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 28/375 00:19 < 04:18, 1.34 it/s, Epoch 0.22/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9s2iHS2yJLp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0908fc508cc84692b403d5eefb31b86a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e3df7040ff4bd48bf0ee990fb61e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a305842d0bd4900afd6310c9cc3c8d1",
              "IPY_MODEL_2d3bad8aa71c4def85cce93a3e7c7d19",
              "IPY_MODEL_88e9212df0684ca38ea5f990b04ecbb6"
            ],
            "layout": "IPY_MODEL_24591885b81d4e289d089f3049d5a886"
          }
        },
        "0a10ed4362fb46b08024ca1e141a6553": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a1708992fd54cf7ab63e80eec3304c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb40c60d89d4b55b4f51e23c15eaf95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2355e46b3df445e3be8280a842b25ae5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24591885b81d4e289d089f3049d5a886": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d35ea464b29419c9be540374cf6b0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a1708992fd54cf7ab63e80eec3304c3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_777035fbf0ba47b68ba142200138bb64",
            "value": "Map: 100%"
          }
        },
        "2d3bad8aa71c4def85cce93a3e7c7d19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0908fc508cc84692b403d5eefb31b86a",
            "max": 4203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bb40c60d89d4b55b4f51e23c15eaf95",
            "value": 4203
          }
        },
        "42780ac5ef1149409aa78d2d8f1dc75d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d35ea464b29419c9be540374cf6b0fa",
              "IPY_MODEL_dcd3b14e89d444768e3bc5340c280f1c",
              "IPY_MODEL_f5c188cc8c0649af84ce15d9771d6161"
            ],
            "layout": "IPY_MODEL_d55e313cdca7482fb67be4be6e6045a8"
          }
        },
        "462e7a0b54024facb7103fa036854e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49114b5ed461432ba67c371b33e30e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49c5424a787240f6a0c540d90e95b17b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c19b7dc46b240c8be1b77f270bb06d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_859ff521bd40497a86d5f85eca49f697",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_50c8d08ce2c6445f83c3b3daaab4a900",
            "value": "Map: 100%"
          }
        },
        "4c1a7c24bdc14387a966b58c4f6ea912": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd4d6e504bc4ad5808c5ec3e041444f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f2c5396287746cc858ee7a10aa0cb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50c8d08ce2c6445f83c3b3daaab4a900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77016231069a4551a958125c5ec1e3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_462e7a0b54024facb7103fa036854e2c",
            "max": 4000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a10ed4362fb46b08024ca1e141a6553",
            "value": 4000
          }
        },
        "777035fbf0ba47b68ba142200138bb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77d5f91908194a3184cfeb16e6fe7d55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859ff521bd40497a86d5f85eca49f697": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88e9212df0684ca38ea5f990b04ecbb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2355e46b3df445e3be8280a842b25ae5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4cd4d6e504bc4ad5808c5ec3e041444f",
            "value": " 4.20k/4.20k [00:00&lt;00:00, 223kB/s]"
          }
        },
        "9a305842d0bd4900afd6310c9cc3c8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c97bd8f0d4a44467911944af98379f7b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a6b8e39d47b34667b1d1096ea0c1a720",
            "value": "Downloading builder script: 100%"
          }
        },
        "a6b8e39d47b34667b1d1096ea0c1a720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae1998e47e6b4f5284b7037827841f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0367179756641e3a80e3a06c2821ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c19b7dc46b240c8be1b77f270bb06d7",
              "IPY_MODEL_77016231069a4551a958125c5ec1e3b5",
              "IPY_MODEL_c3869650664d45279de9b88216ea49fa"
            ],
            "layout": "IPY_MODEL_77d5f91908194a3184cfeb16e6fe7d55"
          }
        },
        "c0597b2d4aa74ce6b7c07d5980236758": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3869650664d45279de9b88216ea49fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0597b2d4aa74ce6b7c07d5980236758",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49114b5ed461432ba67c371b33e30e7c",
            "value": " 4000/4000 [00:01&lt;00:00, 3632.94 examples/s]"
          }
        },
        "c97bd8f0d4a44467911944af98379f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d55e313cdca7482fb67be4be6e6045a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd3b14e89d444768e3bc5340c280f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae1998e47e6b4f5284b7037827841f6d",
            "max": 36000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f2c5396287746cc858ee7a10aa0cb1f",
            "value": 36000
          }
        },
        "f5c188cc8c0649af84ce15d9771d6161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c1a7c24bdc14387a966b58c4f6ea912",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49c5424a787240f6a0c540d90e95b17b",
            "value": " 36000/36000 [00:16&lt;00:00, 3539.50 examples/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
