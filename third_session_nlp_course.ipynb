{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: openai in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (0.27.8)\n",
      "Requirement already satisfied: tqdm in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from aiohttp->openai) (6.0.2)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip install chromadb langchain tabulate google-api-python-client openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ejcv/NLP_course/blob/main/third_session_nlp_course.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## LLMs 🦜\n",
    "After the revolution the attention mechanism and the transformer architecture brought, the next big thing in NLP was the introduction of language models. Language models are models that are trained to predict the next word in a sequence of words. The idea is that if a model can predict the next word in a sequence, it must have learned something about the language and the structure of the text. The first language model was the GPT model, which was trained on a huge dataset, they have existed for a while but it was until the launch of ChatGPT that its popularity exploded.\n",
    "\n",
    "LLMs are a very powerful tool, they can be used for a variety of tasks, such as text generation, text classification, text summarization, and many more. But its power resides when we embed it into our applications. For example, we can use a language model to generate text for a chatbot, or we can use it to classify the sentiment of a text, or we can use it to summarize a text. The possibilities are endless.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to interact with a LLM, you can use it locally, which is hard (it is getting easier and easier though) or you can use it on a cloud service. Once you have that, you can either implement all the abstractions you need to make it more useful, or you can use a framework that handles all of that for you. We will explore both ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPENAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can interact with the llm via the openai api, or its equivalent in google\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Hello! How can I assist you today?\"\n",
      "  },\n",
      "  \"finish_reason\": \"stop\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "print(chat_completion.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets load our data and see how it performs classyfing the data\n",
    "# we will use the same data as in the previous session\n",
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/tweet_emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = df['sentiment'].unique()\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot learning 🥚\n",
    "One of the cool things of LLMs are the emergent abilities, for example a model may not have been trained to do a certain task, but it still can do it. This is the case of few shot learning, where a model can do a task without any prior examples.\n",
    "\n",
    "## Few shot learning 🐥\n",
    "Similarly, few shot learning is the ability of a model to do a task with a few examples. For example, if we want to do sentiment analysis, we can give the model a few examples of positive and negative sentences, and it will be able to do sentiment analysis on new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a sentiment classifier specialiazed assistant. Your job is to classify the sentiment of tweets.\n",
      "\n",
      "I will pass you a tweet and you will give me the corresponding sentiment.\n",
      "\n",
      "The valid sentiments are empty, sadness, enthusiasm, neutral, worry, surprise, love, fun, hate, happiness, boredom, relief, anger.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Thats exactly what we will do\n",
    "\n",
    "# Lets create a prompt for the system\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a sentiment classifier specialiazed assistant. Your job is to classify the sentiment of tweets.\n",
    "\n",
    "I will pass you a tweet and you will give me the corresponding sentiment.\n",
    "\n",
    "The valid sentiments are {\", \".join(sentiments)}.\n",
    "\"\"\"\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text:str) -> str:\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages = [{\n",
    "      \"role\": \"system\", \n",
    "      \"content\": system_prompt  \n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": text\n",
    "      }])\n",
    "\n",
    "    return response.choices[0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                                            1956978410\n",
       "sentiment                                                worry\n",
       "content      Bed!!!!!... its time,..... hope i go to school...\n",
       "Name: 45, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 45\n",
    "df.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worry\n"
     ]
    }
   ],
   "source": [
    "answer = classify_sentiment(df.iloc[idx]['content'])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a sentiment classifier specialiazed assistant. Your job is to classify the sentiment of tweets.\n",
      "\n",
      "I will pass you a tweet and you will give me the corresponding sentiment.\n",
      "\n",
      "The valid sentiments are: \"empty, sadness, enthusiasm, neutral, worry, surprise, love, fun, hate, happiness, boredom, relief, anger.\"\n",
      "\n",
      "Here are some examples:\n",
      "- input: @tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[\n",
      "- your answer: empty\n",
      "\n",
      "- input: Layin n bed with a headache  ughhhh...waitin on your call...\n",
      "- your answer: sadness\n",
      "\n",
      "- input: Funeral ceremony...gloomy friday...\n",
      "- your answer: sadness\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can change the system prompt to have a few examples on how to classify the tweets\n",
    "system_prompt = f\"\"\"\n",
    "You are a sentiment classifier specialiazed assistant. Your job is to classify the sentiment of tweets.\n",
    "\n",
    "I will pass you a tweet and you will give me the corresponding sentiment.\n",
    "\n",
    "The valid sentiments are: \"{\", \".join(sentiments)}.\"\n",
    "\n",
    "Here are some examples:\n",
    "- input: {df.iloc[0]['content']}\n",
    "- your answer: {df.iloc[0]['sentiment']}\n",
    "\n",
    "- input: {df.iloc[1]['content']}\n",
    "- your answer: {df.iloc[1]['sentiment']}\n",
    "\n",
    "- input: {df.iloc[2]['content']}\n",
    "- your answer: {df.iloc[2]['sentiment']}\n",
    "\n",
    "\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('First ever dropped call on my mobile. On a call to @Telstra no less! ( being charged for data even though I have a data pack  )',\n",
       " 'worry')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 100\n",
    "df.iloc[idx]['content'], df.iloc[idx]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worry\n"
     ]
    }
   ],
   "source": [
    "answer = classify_sentiment(df.iloc[idx]['content'])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 🦜⛓\n",
    "Langchain is a framework that abstracts the process of building an application that interacts with a language model. It is designed to be modular and extensible, allowing developers to easily swap out components for their own implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "LangChain provides the Chain interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('datasets/state_of_the_union.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, SentenceTransformerEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
      "\n",
      "As Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit. \n",
      "\n",
      "It’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n",
      "\n",
      "And let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n",
      "\n",
      "Third, support our veterans. \n",
      "\n",
      "Veterans are the best of us. \n",
      "\n",
      "I’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n",
      "\n",
      "My administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n",
      "\n",
      "Our troops in Iraq and Afghanistan faced many dangers.\n",
      "\n",
      "A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n",
      "\n",
      "And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n",
      "\n",
      "We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n",
      "\n",
      "We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n",
      "\n",
      "We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n",
      "\n",
      "We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n",
      "\n",
      "But cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. \n",
      "\n",
      "Danielle says Heath was a fighter to the very end. \n",
      "\n",
      "He didn’t know how to stop fighting, and neither did she. \n",
      "\n",
      "Through her pain she found purpose to demand we do better. \n",
      "\n",
      "Tonight, Danielle—we are. \n",
      "\n",
      "The VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \n",
      "\n",
      "And tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers. \n",
      "\n",
      "I’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \n",
      "\n",
      "And fourth, let’s end cancer as we know it. \n",
      "\n",
      "This is personal to me and Jill, to Kamala, and to so many of you. \n",
      "\n",
      "Cancer is the #2 cause of death in America–second only to heart disease.\n",
      "Human: What did the president say about Ketanji Brown Jackson\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, describing her as one of our nation's top legal minds who will continue Justice Breyer's legacy of excellence.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\",verbose=True)\n",
    "\n",
    "matching_docs = db.similarity_search(query)\n",
    "answer =  chain.run(input_documents=matching_docs, question=query)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_pandas_dataframe_agent(OpenAI(temperature=0), df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_pandas_dataframe_agent(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
    "    df,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `df.shape[0]`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m40000\u001b[0m\u001b[32;1m\u001b[1;3mThere are 40,000 rows in the dataframe.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 40,000 rows in the dataframe.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `df.dtypes`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtweet_id      int64\n",
      "sentiment    object\n",
      "content      object\n",
      "dtype: object\u001b[0m\u001b[32;1m\u001b[1;3mThe column types in the dataframe are as follows:\n",
      "- `tweet_id`: int64\n",
      "- `sentiment`: object\n",
      "- `content`: object\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The column types in the dataframe are as follows:\\n- `tweet_id`: int64\\n- `sentiment`: object\\n- `content`: object'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"what are the columns types?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `df['sentiment'].unique().tolist()`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise', 'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger']\u001b[0m\u001b[32;1m\u001b[1;3mThe unique values in the `sentiment` column are: \"empty\", \"sadness\", \"enthusiasm\", \"neutral\", \"worry\", \"surprise\", \"love\", \"fun\", \"hate\", \"happiness\", \"boredom\", \"relief\", and \"anger\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The unique values in the `sentiment` column are: \"empty\", \"sadness\", \"enthusiasm\", \"neutral\", \"worry\", \"surprise\", \"love\", \"fun\", \"hate\", \"happiness\", \"boredom\", \"relief\", and \"anger\".'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Can you tell me the unique values in the sentiment column?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"PythonREPL\",\n",
    "        func=PythonREPLTool(),\n",
    "        description=\"useful for when you need to test a small piece of code\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "suffix = \"\"\"Begin!\"\n",
    "\n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out the population of Mexico\n",
      "Action: Search\n",
      "Action Input: Mexico population\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMexico 2023 population is estimated at 128,455,567 people at mid year. Mexico population is equivalent to 1.6% of the total world population. Mexico ranks ... Mexico covers 1,972,550 km2 (761,610 sq mi), making it the world's 13th-largest country by area; with a population of over 126 million, it is the 10th-most- ... Mexico. Demographic data as of July 1, 2023, economic data for 2022 (source) Print Share ... Current and Projected Population. Population, total - Mexico from The World Bank: Data. ... Population and Vital Statistics Reprot ( various years ), ( 5 ) U.S. Census Bureau: International ... Since 1979, the Population Council has worked in Mexico to improve reproductive health through high-quality research, program evaluation, and technical ... Dec 6, 2016 ... Estimates show that the total Mexican population will reach 150 million in 2050 while the group 65 years and older reaches almost 28.7 million ( ... Dec 6, 2016 ... Figure 1 presents a population pyramid that illustrates the age and sex structure of Mexico's population. Estimates show that the total Mexican ... Feb 28, 2023 ... For the first time since reintroduction into the wild, the population of Mexican wolves in Arizona and New Mexico has surpassed 200 with a ... Jul 29, 2022 ... In this infographic, Jazmín Aguilar Rangel provides a demographic overview of the Afro-Mexican community. She describes certain challenges ... total: 98.8 million (2021 est.) percent of population: 76% (2021 est.) comparison ranking: total 9 · Broadband - fixed subscriptions. total: ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Mexico's population is estimated at 128,455,567 people as of mid-2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Mexico's population is estimated at 128,455,567 people as of mid-2023.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"What is Mexico population?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the population in 2028\n",
      "Action: PythonREPL\n",
      "Action Input: 128455667 * 1.02 ** 5\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 134,845,945 people\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'134,845,945 people'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"if the growth rate of the population is 2% yearly, what will the population be in 2028 (now is 2023)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out what version of Python I'm running\n",
      "Action: PythonREPL\n",
      "Action Input: print(sys.version)\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: I am running Python version 3.10.6.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am running Python version 3.10.6.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"What python version are you running?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
