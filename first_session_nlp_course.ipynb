{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (1.23.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: jinja2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (66.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (66.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "NLP is a subfield of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech.\n",
    "\n",
    "NLP has three main components: **morphological analysis**, **syntactic analysis**, and **semantic analysis**. Morphological analysis breaks words into their basic components, syntactic analysis examines how words are put together to form sentences, and semantic analysis focuses on the meaning of the words and sentences.\n",
    "\n",
    "NLP is used in many different fields, including machine translation, document summarization, speech recognition, topic segmentation, relationship extraction, question answering, sentiment analysis, and spam filtering.\n",
    "\n",
    "In this first session we will cover the following topics:\n",
    "\n",
    "- **Tokenization**: Splitting text into tokens (words, punctuation marks, etc.)\n",
    "- **N-Grams**: Sequences of N tokens\n",
    "- **Part-of-speech tagging**: Identifying the part of speech for each token\n",
    "- **Stopword removal**: Filtering out \"meaningless\" words\n",
    "- **Stemming and lemmatization**: Reducing related words to a common stem or lemma\n",
    "- **Bag of words**: Converting text into the bag-of-words format\n",
    "- **TF-IDF**: Computing word importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Computers dont understand text! (including chatgpt ü§ñ)\n",
    "\n",
    "Computers only understand numbers. We need to convert text to numbers to do anything useful with it. This is called **feature extraction** or **feature encoding**.\n",
    "\n",
    "## ü§î How do we convert text to numbers?\n",
    "\n",
    "There are many ways to do this, but the most common approaches are:\n",
    "\n",
    "- **Bag of words**: Represent the text as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity.\n",
    "- **TF-IDF**: Weigh words by how often they appear in the text, discounting words that appear frequently across all texts.\n",
    "\n",
    "But before we even do that, we need to \"clean\" the text by removing stopwords, punctuation, and other \"noise\". This step is called **text preprocessing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of preprocessing üß±üß±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization in NLP\n",
    "\n",
    "Word tokenization is a crucial preprocessing step in Natural Language Processing (NLP) that involves dividing a given text or sentence into individual words or tokens. This process is essential because most NLP algorithms and models rely on text representations as sequences of words or tokens to analyze and understand the language.\n",
    "\n",
    "The steps involved in word tokenization are as follows:\n",
    "\n",
    "1. **Input Text:** Begin with a piece of text, which can be a sentence, paragraph, or an entire document, depending on the NLP task.\n",
    "\n",
    "2. **Tokenization:** Tokenize the input text into individual words or tokens using various methods, such as space-based tokenization, punctuation-based tokenization, rule-based tokenization, or language-specific tokenization.\n",
    "\n",
    "3. **Handling Special Cases:** Address special cases, such as abbreviations, acronyms, or compound words, to ensure proper token segmentation.\n",
    "\n",
    "4. **Normalization:** Optionally, normalize tokens to lowercase for case-insensitivity or handling word variations.\n",
    "\n",
    "Word tokenization enables us to process and analyze text on a word-level basis, making it possible to perform various NLP tasks, including sentiment analysis, named entity recognition, machine translation, and more. Proper tokenization ensures meaningful representations of language data that can be utilized as input for different NLP algorithms and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Welcome to ChatGPT, an AI language model powered by OpenAI. This is a demo of the model's capabilities. \n",
    "It can understand various languages, including English, Spanish, and French. Don't hesitate to ask questions and use contractions like can't and won't. \n",
    "By the way, this sentence contains a hyphenated word: state-of-the-art. AI is fascinating and full of surprises! Make sure to handle acronyms like USA and NASA properly.\n",
    "\"\"\"\n",
    "\n",
    "# We can see the text includes several sentences, and it contains a hyphenated word, a contraction, and an acronym. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive solution ü´£\n",
    "Using the split() method from python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['This',\n",
       "  'is',\n",
       "  'a',\n",
       "  'demo',\n",
       "  'of',\n",
       "  'the',\n",
       "  \"model's\",\n",
       "  'capabilities.',\n",
       "  '\\nIt',\n",
       "  'can'],\n",
       " ['Make',\n",
       "  'sure',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'acronyms',\n",
       "  'like',\n",
       "  'USA',\n",
       "  'and',\n",
       "  'NASA',\n",
       "  'properly.\\n'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(' ')[10:20],text.split(' ')[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more serious approach\n",
    "We can first clean the text a little bit, and then split it into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\" A function that gets rid of punctuation, \n",
    "    special characters and extra spaces. \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Welcome',\n",
       "  'to',\n",
       "  'ChatGPT',\n",
       "  'an',\n",
       "  'AI',\n",
       "  'language',\n",
       "  'model',\n",
       "  'powered',\n",
       "  'by',\n",
       "  'OpenAI'],\n",
       " ['Make',\n",
       "  'sure',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'acronyms',\n",
       "  'like',\n",
       "  'USA',\n",
       "  'and',\n",
       "  'NASA',\n",
       "  'properly'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(text).split(' ')[:10], preprocess_text(text).split( )[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still facing some problems, like compound words, abbreviations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK library\n",
    "We can continue complicating the process and adding more rules, but we will never be able to cover all the cases. Luckily, there are libraries that can help us with this task.\n",
    "\n",
    "The NLTK library includes several tokenizers that can help us with this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Welcome',\n",
       "  'to',\n",
       "  'ChatGPT',\n",
       "  'an',\n",
       "  'AI',\n",
       "  'language',\n",
       "  'model',\n",
       "  'powered',\n",
       "  'by',\n",
       "  'OpenAI'],\n",
       " ['to',\n",
       "  'ask',\n",
       "  'questions',\n",
       "  'and',\n",
       "  'use',\n",
       "  'contractions',\n",
       "  'like',\n",
       "  'cant',\n",
       "  'and',\n",
       "  'wont',\n",
       "  'By',\n",
       "  'the',\n",
       "  'way',\n",
       "  'this',\n",
       "  'sentence'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\" A function that tokenizes text into words. \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "tokenize_text(preprocess_text(text))[:10], tokenize_text(preprocess_text(text))[30:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK uses a model called PunktSentenceTokenizer. This model is trained to tokenize sentences or words in a given language. NLTK also includes a pre-trained PunktSentenceTokenizer for English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we are not done yet, let's say we want to count the times a word appears in a text. We need to make sure that we are counting the same word, and not different variations of it.\n",
    "\n",
    "That is where lemmatisation and stemming come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation ü•ä Stemming\n",
    "\n",
    "Lemmatisation and stemming are two popular text normalization techniques that are used to prepare text for further processing. Both techniques are used to reduce the inflectional forms of words to their root forms so that they can be analyzed as a single item, rather than as multiple words.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **Stemming:** The root form of the word \"arguing\" is \"argu\", and the root form of the word \"argument\" is \"argument\".\n",
    "- **Lemmatisation:** The root form of the word \"arguing\" is \"argue\", and the root form of the word \"argument\" is \"argument\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([',',\n",
       "  'spanish',\n",
       "  ',',\n",
       "  'and',\n",
       "  'french',\n",
       "  '.',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'hesit',\n",
       "  'to',\n",
       "  'ask',\n",
       "  'question',\n",
       "  'and',\n",
       "  'use',\n",
       "  'contract'],\n",
       " [',',\n",
       "  'Spanish',\n",
       "  ',',\n",
       "  'and',\n",
       "  'French',\n",
       "  '.',\n",
       "  'Do',\n",
       "  \"n't\",\n",
       "  'hesitate',\n",
       "  'to',\n",
       "  'ask',\n",
       "  'question',\n",
       "  'and',\n",
       "  'use',\n",
       "  'contraction'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets compare stemming and lemmatisation\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def stem_text(tokens: list) -> list:\n",
    "    \"\"\" A function that stems text. \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatize_text(tokens: list) -> list:\n",
    "    \"\"\" A function that lemmatizes text. \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "stem_text(tokenize_text(text))[30:45], lemmatize_text(tokenize_text(text))[30:45]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sure',\n",
       "  'to',\n",
       "  'handl',\n",
       "  'acronym',\n",
       "  'like',\n",
       "  'usa',\n",
       "  'and',\n",
       "  'nasa',\n",
       "  'properli',\n",
       "  '.'],\n",
       " ['sure',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'acronym',\n",
       "  'like',\n",
       "  'USA',\n",
       "  'and',\n",
       "  'NASA',\n",
       "  'properly',\n",
       "  '.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text(tokenize_text(text))[-10:], lemmatize_text(tokenize_text(text))[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the differences between the two techniques. Stemming is a crude method that simply chops off the end of a word using heuristics, without taking into account the context of the word. Lemmatisation, on the other hand, uses a more informed analysis to create groups of words with similar meaning based on the context around the word.\n",
    "\n",
    "But lemmatisation can get things wrong without context, for example, the word \"better\" can be a comparative adjective or an adverb, and the lemma of \"better\" is \"good\". So, if we lemmatize the word \"better\" without context, we will get \"good\" as the lemma, which is not correct. This is were Part-of-speech tagging comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech Tagging\n",
    "\n",
    "Part-of-speech tagging is the process of assigning a part-of-speech tag to each word in a given text (corpus). A part-of-speech tag is a special label assigned to each token (word) in a text corpus to indicate the part of speech and often also other grammatical categories such as tense, number (plural/singular), case etc. Part-of-speech tagging is also known as grammatical tagging or word-category disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', ','),\n",
       " ('Spanish', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('French', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('hesitate', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('ask', 'VB'),\n",
       " ('questions', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('use', 'VBP'),\n",
       " ('contractions', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('ca', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('wo', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('.', '.'),\n",
       " ('By', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do pos tagging and then lemmatisation\n",
    "from nltk import pos_tag\n",
    "\n",
    "def pos_tag_text(tokens: list) -> list:\n",
    "    \"\"\" A function that tags text with parts of speech. \"\"\"\n",
    "    return pos_tag(tokens)\n",
    "\n",
    "tagged_tokens = pos_tag_text(tokenize_text(text))\n",
    "tagged_tokens[30:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a table with the different tags meanings [here](https://www.guru99.com/pos-tagging-chunking-nltk.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'VB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb Cell 24\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [lemmatizer\u001b[39m.\u001b[39mlemmatize(token, tag) \u001b[39mif\u001b[39;00m tag \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m token \u001b[39mfor\u001b[39;00m token, tag \u001b[39min\u001b[39;00m tagged_tokens ]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m lemmatize_tagged_text(tagged_tokens)[\u001b[39m30\u001b[39m:\u001b[39m45\u001b[39m]\n",
      "\u001b[1;32m/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb Cell 24\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\"\"\" A function that lemmatizes text based on its parts of speech. \"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [lemmatizer\u001b[39m.\u001b[39mlemmatize(token, tag) \u001b[39mif\u001b[39;00m tag \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m token \u001b[39mfor\u001b[39;00m token, tag \u001b[39min\u001b[39;00m tagged_tokens ]\n",
      "\u001b[1;32m/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb Cell 24\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\"\"\" A function that lemmatizes text based on its parts of speech. \"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ernesto/Documents/Work/personal/NLPcourse/first_session_nlp_course.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [lemmatizer\u001b[39m.\u001b[39;49mlemmatize(token, tag) \u001b[39mif\u001b[39;00m tag \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m token \u001b[39mfor\u001b[39;00m token, tag \u001b[39min\u001b[39;00m tagged_tokens ]\n",
      "File \u001b[0;32m~/miniforge3/envs/hackathon/lib/python3.10/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/miniforge3/envs/hackathon/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:2008\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_morphy\u001b[39m(\u001b[39mself\u001b[39m, form, pos, check_exceptions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   2001\u001b[0m     \u001b[39m# from jordanbg:\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m     \u001b[39m# Given an original string x\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2005\u001b[0m     \u001b[39m# 3. If there are no matches, keep applying rules until you either\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     \u001b[39m#    find a match or you can't go any further\u001b[39;00m\n\u001b[0;32m-> 2008\u001b[0m     exceptions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exception_map[pos]\n\u001b[1;32m   2009\u001b[0m     substitutions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001b[1;32m   2011\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'VB'"
     ]
    }
   ],
   "source": [
    "# now that they are tagged we can use the tags to lemmatise\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatize_tagged_text(tagged_tokens: list) -> list:\n",
    "    \"\"\" A function that lemmatizes text based on its parts of speech. \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token, tag) if tag != None else token for token, tag in tagged_tokens ]\n",
    "\n",
    "lemmatize_tagged_text(tagged_tokens)[30:45]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', None),\n",
       " ('Spanish', 'n'),\n",
       " (',', None),\n",
       " ('and', None),\n",
       " ('French', 'n'),\n",
       " ('.', None),\n",
       " ('Do', 'v'),\n",
       " (\"n't\", 'r'),\n",
       " ('hesitate', 'v'),\n",
       " ('to', None),\n",
       " ('ask', 'v'),\n",
       " ('questions', 'n'),\n",
       " ('and', None),\n",
       " ('use', 'v'),\n",
       " ('contractions', 'n'),\n",
       " ('like', None),\n",
       " ('ca', None),\n",
       " (\"n't\", 'r'),\n",
       " ('and', None),\n",
       " ('wo', None),\n",
       " (\"n't\", 'r'),\n",
       " ('.', None),\n",
       " ('By', None),\n",
       " ('the', None),\n",
       " ('way', 'n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "wordnet_compt_tokens = [(tok, get_wordnet_pos(tag))for tok, tag in tagged_tokens]\n",
    "wordnet_compt_tokens[30:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'Spanish',\n",
       " ',',\n",
       " 'and',\n",
       " 'French',\n",
       " '.',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'question',\n",
       " 'and',\n",
       " 'use',\n",
       " 'contraction']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_tagged_text(wordnet_compt_tokens)[30:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another library to do Pos tagging is Spacy \n",
    "Spacy is a library for NLP that has a lot of the features we have seen so far, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI OpenAI PROPN NNP pobj XxxxXX True False\n",
      ". . PUNCT . punct . False False\n",
      "This this PRON DT nsubj Xxxx True True\n",
      "is be AUX VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "demo demo NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "model model NOUN NN poss xxxx True False\n",
      "'s 's PART POS case 'x False True\n",
      "capabilities capability NOUN NNS pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "\n",
      " \n",
      " SPACE _SP dep \n",
      " False False\n",
      "It it PRON PRP nsubj Xx True True\n",
      "can can AUX MD aux xxx True True\n",
      "understand understand VERB VB ROOT xxxx True False\n",
      "various various ADJ JJ amod xxxx True True\n",
      "languages language NOUN NNS dobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "including include VERB VBG prep xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc[10:30]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise classify a document\n",
    "\n",
    "Now that we have seen some of the basic NLP preprocessing techniques, let's try to classify some text.\n",
    "\n",
    "We downloaded this [dataset](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download) from Kaggle. It contains a set of tweets labeled with the emotion they express.\n",
    "\n",
    "## Bag of words üõçÔ∏è\n",
    "\n",
    "We will start with a simple (yet powerful) technique called Bag of Words. This technique consists of counting the number of times a word appears in a document. We can use this technique to convert a document into a vector of numbers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/tweet_emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first step would be to assign a numeric value to each sentiment\n",
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'empty',\n",
       " 1: 'sadness',\n",
       " 2: 'enthusiasm',\n",
       " 3: 'neutral',\n",
       " 4: 'worry',\n",
       " 5: 'surprise',\n",
       " 6: 'love',\n",
       " 7: 'fun',\n",
       " 8: 'hate',\n",
       " 9: 'happiness',\n",
       " 10: 'boredom',\n",
       " 11: 'relief',\n",
       " 12: 'anger'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we enumerate the unique values and then convert it to a dictionary where the key is the index and the value is the sentiment\n",
    "id2sent = dict(enumerate(df['sentiment'].unique()))\n",
    "sent2id = {v:k for k,v in id2sent.items()}\n",
    "id2sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "   sentiment_id  \n",
       "0             0  \n",
       "1             1  \n",
       "2             1  \n",
       "3             2  \n",
       "4             3  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can create a column with the numeric values\n",
    "df['sentiment_id'] = df['sentiment'].map(sent2id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@mrspaulkjonas No, havent heard anything about the show   It hasnt been on on saturday nights, but there are the occasional sunday reruns.\n",
      "neutral\n",
      "----------------------\n",
      "Happy Mother's Day @shannon180\n",
      "love\n",
      "----------------------\n",
      "@danachs I am lost. Please help me find a good home.\n",
      "worry\n",
      "----------------------\n",
      "@laurenhobeast As much as I loved Texas, my dislike for needles could interfere.\n",
      "love\n",
      "----------------------\n",
      "@exorre Our raid leader made us run drills to practice switching\n",
      "worry\n",
      "----------------------\n",
      "loves twitter (Y) 38th post. mwaha  x\n",
      "love\n",
      "----------------------\n",
      "@smartie999 That's me on a good day\n",
      "sadness\n",
      "----------------------\n",
      "@DutchReaganite So am I.\n",
      "neutral\n",
      "----------------------\n",
      "CRYING CUZ PEOPLE WONT FOLLOW ME!!!!!!!\n",
      "worry\n",
      "----------------------\n",
      "I'm scared to set the pw on my BB bc I might forget it &amp; end up losing all my data. But yet I have financial info on it. Catch 22\n",
      "worry\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets apply what we learnt so far to preprocess the text\n",
    "# the first step would be to explore the different types of text we have\n",
    "# lets get a random sample and see what we have\n",
    "for _, row in df.sample(10).iterrows():\n",
    "    print(row['content'])\n",
    "    print(row['sentiment'])\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hello world '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see there are usernames that we dont need, a mix of lower and uppercase, some special characters (parenthesis, exclamations and question marks, etc), triple dots, abbreviations, etc\n",
    "# lets start by removing the usernames\n",
    "import re\n",
    "def remove_usernames(text: str) -> str:\n",
    "    \"\"\" A function that removes usernames from text. \"\"\"\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "remove_usernames('@user hello world @user') # test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world this is a bad str ng 2023 is the year'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets remove special characters, triple dots, double spaces, punctuation, etc\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\" A function that gets rid of punctuation, \n",
    "    special characters and extra spaces. \n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.lower() # turn everything to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
    "\n",
    "    return text\n",
    "\n",
    "preprocess_text('HeLlo.World! #this is;, (a bad str!ng)...    2023 is the year') # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was listenin to bad habit earlier an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty   i know i was listenin to bad habit earlier an...   \n",
       "1  1956967666     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
       "2  1956967696     sadness                    funeral ceremony gloomy friday    \n",
       "3  1956967789  enthusiasm               wants to hang out with friends soon    \n",
       "4  1956968416     neutral   we want to trade with someone who has houston...   \n",
       "\n",
       "   sentiment_id  \n",
       "0             0  \n",
       "1             1  \n",
       "2             1  \n",
       "3             2  \n",
       "4             3  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with what we have learnt so far we cant handle abbreviations, we won't cover that here\n",
    "# but if interested you can search for grammar correctors, using an exhaustive dictionary, or even a probabilistic model\n",
    "\n",
    "# lets put everything together and apply it to the dataframe\n",
    "df['content'] = df['content'].apply(remove_usernames).apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'be', 'run', 'and', 'eat', 'while', 'i', 'write', 'a', 'poem']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have preprocessed the text we can tokenize it\n",
    "# we can use a simple tokenizer from nltk and then lemmatize the tokens, so we can reduce the number of tokens \n",
    "# words like 'running' and 'run' will be converted to 'run' and grouped together\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\" A function that tokenizes text into words. \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tagged_tokens = pos_tag(tokens)\n",
    "    wordnet_compt_tokens = [(tok, get_wordnet_pos(tag))for tok, tag in pos_tagged_tokens]\n",
    "    lemmatized_tokens = lemmatize_tagged_text(wordnet_compt_tokens)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "tokenize_text('i am running and eating while i write a poem') # test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was listenin to bad habit earlier an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, know, i, be, listenin, to, bad, habit, ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "      <td>1</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>2</td>\n",
       "      <td>[want, to, hang, out, with, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>3</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, have...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty   i know i was listenin to bad habit earlier an...   \n",
       "1  1956967666     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
       "2  1956967696     sadness                    funeral ceremony gloomy friday    \n",
       "3  1956967789  enthusiasm               wants to hang out with friends soon    \n",
       "4  1956968416     neutral   we want to trade with someone who has houston...   \n",
       "\n",
       "   sentiment_id                                             tokens  \n",
       "0             0  [i, know, i, be, listenin, to, bad, habit, ear...  \n",
       "1             1  [layin, n, bed, with, a, headache, ughhhh, wai...  \n",
       "2             1                [funeral, ceremony, gloomy, friday]  \n",
       "3             2          [want, to, hang, out, with, friend, soon]  \n",
       "4             3  [we, want, to, trade, with, someone, who, have...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# lets apply the tokenizer to the dataframe\n",
    "df['tokens'] = df['content'].apply(tokenize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is an extra preprocessing technique we can use, that we haven't seen yet, and that is stop words removal\n",
    "# Stop words are words that are very common in a language and don't add much meaning to a sentence\n",
    "# Examples of stop words are 'the', 'a', 'an', 'and', 'or', etc\n",
    "# We can use the stop words from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'eat', 'write', 'poem']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do this after tokenizing the text, because the pos tagging works better with these words as context, it can tell if the word is a verb or a noun, etc\n",
    "def remove_stop_words(tokens: list) -> list:\n",
    "    \"\"\" A function that removes stop words from tokens. \"\"\"\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "remove_stop_words(tokenize_text('i am running and eating while i write a poem')) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was listenin to bad habit earlier an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[know, listenin, bad, habit, earlier, start, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "      <td>1</td>\n",
       "      <td>[layin, n, bed, headache, ughhhh, waitin, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>2</td>\n",
       "      <td>[want, hang, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>3</td>\n",
       "      <td>[want, trade, someone, houston, ticket, one]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty   i know i was listenin to bad habit earlier an...   \n",
       "1  1956967666     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
       "2  1956967696     sadness                    funeral ceremony gloomy friday    \n",
       "3  1956967789  enthusiasm               wants to hang out with friends soon    \n",
       "4  1956968416     neutral   we want to trade with someone who has houston...   \n",
       "\n",
       "   sentiment_id                                             tokens  \n",
       "0             0  [know, listenin, bad, habit, earlier, start, f...  \n",
       "1             1    [layin, n, bed, headache, ughhhh, waitin, call]  \n",
       "2             1                [funeral, ceremony, gloomy, friday]  \n",
       "3             2                         [want, hang, friend, soon]  \n",
       "4             3       [want, trade, someone, houston, ticket, one]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets apply the stop words removal to the dataframe\n",
    "df['tokens'] = df['tokens'].apply(remove_stop_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'be', 'run', 'and', 'eat', 'while', 'I', 'write', 'a', 'poem'],\n",
       " ['I',\n",
       "  'be',\n",
       "  'a',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'who',\n",
       "  'like',\n",
       "  'to',\n",
       "  'run',\n",
       "  'in',\n",
       "  'the',\n",
       "  'park',\n",
       "  'with',\n",
       "  'my',\n",
       "  'dog'],\n",
       " ['I', 'enjoy', 'watch', 'movie', 'with', 'my', 'friend']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have preprocessed the text we can start building our model\n",
    "# we will use a simple bag of words model, where we will count the number of times a word appears in a sentence\n",
    "\n",
    "\n",
    "# lets do it manually first\n",
    "# we will create a dictionary where the key is the word and the value is the number of times it appears in the sentence\n",
    "# we will use the first sentence as an example\n",
    "# lets suppose we have a corpus of 3 sentences\n",
    "corpus = ['I am running and eating while I write a poem', 'I am a data scientist who likes to run in the park with my dog', 'I enjoy watching movies with my friends']\n",
    "\n",
    "tokens_lst = [tokenize_text(sentence) for sentence in corpus]\n",
    "tokens_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'who': 0,\n",
       " 'run': 1,\n",
       " 'park': 2,\n",
       " 'friend': 3,\n",
       " 'in': 4,\n",
       " 'and': 5,\n",
       " 'write': 6,\n",
       " 'movie': 7,\n",
       " 'watch': 8,\n",
       " 'poem': 9,\n",
       " 'with': 10,\n",
       " 'dog': 11,\n",
       " 'while': 12,\n",
       " 'enjoy': 13,\n",
       " 'a': 14,\n",
       " 'to': 15,\n",
       " 'like': 16,\n",
       " 'my': 17,\n",
       " 'I': 18,\n",
       " 'eat': 19,\n",
       " 'the': 20,\n",
       " 'be': 21,\n",
       " 'data': 22,\n",
       " 'scientist': 23}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a dictionary where the key is the word and the value is the number of times it appears in the sentence\n",
    "# the vocabulary will be the set of unique words in the corpus\n",
    "vocab ={token: idx for idx, token in enumerate(set([token for tokens in tokens_lst for token in tokens]))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0],\n",
       " [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
       " [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a vector for each sentence, where the length of the vector is the length of the vocabulary\n",
    "# the value of each element in the vector will be the number of times the word appears in the sentence\n",
    "bow = [[token.count(word) for word in vocab] for token in tokens_lst]\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0,\n",
       " 'running': 14,\n",
       " 'and': 1,\n",
       " 'eating': 4,\n",
       " 'while': 19,\n",
       " 'write': 22,\n",
       " 'poem': 12,\n",
       " 'data': 2,\n",
       " 'scientist': 15,\n",
       " 'who': 20,\n",
       " 'likes': 8,\n",
       " 'to': 17,\n",
       " 'run': 13,\n",
       " 'in': 7,\n",
       " 'the': 16,\n",
       " 'park': 11,\n",
       " 'with': 21,\n",
       " 'my': 10,\n",
       " 'dog': 3,\n",
       " 'enjoy': 5,\n",
       " 'watching': 18,\n",
       " 'movies': 9,\n",
       " 'friends': 6}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see the vocab takes all the unique words in the corpus and assigns a number to each one, and then we go through \n",
    "# each sentence and count the number of times each word appears in the sentence\n",
    "\n",
    "# this is a very time consuming process, fortunately, we have libraries that can do this for us\n",
    "# we will use the CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# we will use the same corpus as before\n",
    "\n",
    "# we will create an instance of the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# we will fit the vectorizer to the corpus\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# we will get the vocabulary\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will transform the corpus into a bag of words\n",
    "bow = vectorizer.transform(corpus)\n",
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28123"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see the \"I\" was removed. The count vectorizer implementation removes stop words by default, and does tokenization\n",
    "# We already have our text preprocessed, so we will turn off the tokenization and stop words removal\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x,lowercase=False, token_pattern=None)\n",
    "\n",
    "\n",
    "\n",
    "# we will fit the vectorizer to the corpus\n",
    "# but now the corpus is the list of tokens in the dataframe\n",
    "vectorizer.fit(df['tokens'].tolist())\n",
    "\n",
    "\n",
    "# we will get the vocabulary size\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was listenin to bad habit earlier an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[know, listenin, bad, habit, earlier, start, f...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "      <td>1</td>\n",
       "      <td>[layin, n, bed, headache, ughhhh, waitin, call]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>2</td>\n",
       "      <td>[want, hang, friend, soon]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>3</td>\n",
       "      <td>[want, trade, someone, houston, ticket, one]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty   i know i was listenin to bad habit earlier an...   \n",
       "1  1956967666     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
       "2  1956967696     sadness                    funeral ceremony gloomy friday    \n",
       "3  1956967789  enthusiasm               wants to hang out with friends soon    \n",
       "4  1956968416     neutral   we want to trade with someone who has houston...   \n",
       "\n",
       "   sentiment_id                                             tokens  \\\n",
       "0             0  [know, listenin, bad, habit, earlier, start, f...   \n",
       "1             1    [layin, n, bed, headache, ughhhh, waitin, call]   \n",
       "2             1                [funeral, ceremony, gloomy, friday]   \n",
       "3             2                         [want, hang, friend, soon]   \n",
       "4             3       [want, trade, someone, houston, ticket, one]   \n",
       "\n",
       "                                                 bow  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will transform the corpus into a bag of words\n",
    "# important, we need to wrap the tokens in a list, this is because we turned off the preprocessing and tokenization \n",
    "df['bow'] = df['tokens'].apply(lambda x: vectorizer.transform([x]).toarray()[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have the bag of words we can start building our model\n",
    "# we will use a simple logistic regression model for multiclass\n",
    "# we will use the bow as the input and the sentiment_id as the target\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for demo purposes we won't use the whole dataset\n",
    "sample_df = df.sample(10000)\n",
    "\n",
    "# we will split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['bow'].tolist(), sample_df['sentiment_id'].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "# we will create an instance of the model\n",
    "model = LogisticRegression(solver='saga', n_jobs=-1)\n",
    "\n",
    "# we will fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# we will get the predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# we will get the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
