{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (21.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (66.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ernesto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ernesto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ernesto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ernesto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install spacy --quiet\n",
    "! python3 -m spacy download en_core_web_sm \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.redd.it/mkj6a0p8s8391.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ejcv/NLP_course/blob/main/first_session_nlp_course.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "NLP is a subfield of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech.\n",
    "\n",
    "NLP has three main components: **morphological analysis**, **syntactic analysis**, and **semantic analysis**. Morphological analysis breaks words into their basic components, syntactic analysis examines how words are put together to form sentences, and semantic analysis focuses on the meaning of the words and sentences.\n",
    "\n",
    "NLP is used in many different fields, including machine translation, document summarization, speech recognition, topic segmentation, relationship extraction, question answering, sentiment analysis, and spam filtering.\n",
    "\n",
    "In this first session we will cover the following topics:\n",
    "\n",
    "- **Tokenization**: Splitting text into tokens (words, punctuation marks, etc.)\n",
    "- **N-Grams**: Sequences of N tokens\n",
    "- **Part-of-speech tagging**: Identifying the part of speech for each token\n",
    "- **Stopword removal**: Filtering out \"meaningless\" words\n",
    "- **Stemming and lemmatization**: Reducing related words to a common stem or lemma\n",
    "- **Bag of words**: Converting text into the bag-of-words format\n",
    "- **TF-IDF**: Computing word importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Computers dont understand text! (including chatgpt ü§ñ)\n",
    "\n",
    "Computers only understand numbers. We need to convert text to numbers to do anything useful with it. This is called **feature extraction** or **feature encoding**.\n",
    "\n",
    "## ü§î How do we convert text to numbers?\n",
    "\n",
    "There are many ways to do this, but the most common approaches are:\n",
    "\n",
    "- **Bag of words**: Represent the text as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity.\n",
    "- **TF-IDF**: Weigh words by how often they appear in the text, discounting words that appear frequently across all texts.\n",
    "\n",
    "But before we even do that, we need to \"clean\" the text by removing stopwords, punctuation, and other \"noise\". This step is called **text preprocessing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of preprocessing üß±üß±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization in NLP\n",
    "\n",
    "Word tokenization is a crucial preprocessing step in Natural Language Processing (NLP) that involves dividing a given text or sentence into individual words or tokens. This process is essential because most NLP algorithms and models rely on text representations as sequences of words or tokens to analyze and understand the language.\n",
    "\n",
    "The steps involved in word tokenization are as follows:\n",
    "\n",
    "1. **Input Text:** Begin with a piece of text, which can be a sentence, paragraph, or an entire document, depending on the NLP task.\n",
    "\n",
    "2. **Tokenization:** Tokenize the input text into individual words or tokens using various methods, such as space-based tokenization, punctuation-based tokenization, rule-based tokenization, or language-specific tokenization.\n",
    "\n",
    "3. **Handling Special Cases:** Address special cases, such as abbreviations, acronyms, or compound words, to ensure proper token segmentation.\n",
    "\n",
    "4. **Normalization:** Optionally, normalize tokens to lowercase for case-insensitivity or handling word variations.\n",
    "\n",
    "Word tokenization enables us to process and analyze text on a word-level basis, making it possible to perform various NLP tasks, including sentiment analysis, named entity recognition, machine translation, and more. Proper tokenization ensures meaningful representations of language data that can be utilized as input for different NLP algorithms and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Welcome to ChatGPT, an AI language model powered by OpenAI. This is a demo of the model's capabilities. \n",
    "It can understand various languages, including English, Spanish, and French. Don't hesitate to ask questions and use contractions like can't and won't. \n",
    "By the way, this sentence contains a hyphenated word: state-of-the-art. AI is fascinating and full of surprises! Make sure to handle acronyms like USA and NASA properly.\n",
    "\"\"\"\n",
    "\n",
    "# We can see the text includes several sentences, and it contains a hyphenated word, a contraction, and an acronym. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive solution ü´£\n",
    "Using the split() method from python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['This',\n",
       "  'is',\n",
       "  'a',\n",
       "  'demo',\n",
       "  'of',\n",
       "  'the',\n",
       "  \"model's\",\n",
       "  'capabilities.',\n",
       "  '\\nIt',\n",
       "  'can'],\n",
       " ['Make',\n",
       "  'sure',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'acronyms',\n",
       "  'like',\n",
       "  'USA',\n",
       "  'and',\n",
       "  'NASA',\n",
       "  'properly.\\n'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(' ')[10:20],text.split(' ')[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more serious approach\n",
    "We can first clean the text a little bit, and then split it into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\" A function that gets rid of punctuation, \n",
    "    special characters and extra spaces. \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Welcome',\n",
       "  'to',\n",
       "  'ChatGPT',\n",
       "  'an',\n",
       "  'AI',\n",
       "  'language',\n",
       "  'model',\n",
       "  'powered',\n",
       "  'by',\n",
       "  'OpenAI'],\n",
       " ['Make',\n",
       "  'sure',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'acronyms',\n",
       "  'like',\n",
       "  'USA',\n",
       "  'and',\n",
       "  'NASA',\n",
       "  'properly'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(text).split(' ')[:10], preprocess_text(text).split( )[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still facing some problems, like compound words, abbreviations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK library\n",
    "We can continue complicating the process and adding more rules, but we will never be able to cover all the cases. Luckily, there are libraries that can help us with this task.\n",
    "\n",
    "The NLTK library includes several tokenizers that can help us with this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Welcome',\n",
       "  'to',\n",
       "  'ChatGPT',\n",
       "  'an',\n",
       "  'AI',\n",
       "  'language',\n",
       "  'model',\n",
       "  'powered',\n",
       "  'by',\n",
       "  'OpenAI'],\n",
       " ['to',\n",
       "  'ask',\n",
       "  'questions',\n",
       "  'and',\n",
       "  'use',\n",
       "  'contractions',\n",
       "  'like',\n",
       "  'cant',\n",
       "  'and',\n",
       "  'wont',\n",
       "  'By',\n",
       "  'the',\n",
       "  'way',\n",
       "  'this',\n",
       "  'sentence'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\" A function that tokenizes text into words. \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "tokenize_text(preprocess_text(text))[:10], tokenize_text(preprocess_text(text))[30:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK uses a model called PunktSentenceTokenizer. This model is trained to tokenize sentences or words in a given language. NLTK also includes a pre-trained PunktSentenceTokenizer for English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we are not done yet, let's say we want to count the times a word appears in a text. We need to make sure that we are counting the same word, and not different variations of it.\n",
    "\n",
    "That is where lemmatisation and stemming come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation ü•ä Stemming\n",
    "\n",
    "Lemmatisation and stemming are two popular text normalization techniques that are used to prepare text for further processing. Both techniques are used to reduce the inflectional forms of words to their root forms so that they can be analyzed as a single item, rather than as multiple words.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **Stemming:** The root form of the word \"arguing\" is \"argu\", and the root form of the word \"argument\" is \"argument\".\n",
    "- **Lemmatisation:** The root form of the word \"arguing\" is \"argue\", and the root form of the word \"argument\" is \"argument\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([',',\n",
       "  'spanish',\n",
       "  ',',\n",
       "  'and',\n",
       "  'french',\n",
       "  '.',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'hesit',\n",
       "  'to',\n",
       "  'ask',\n",
       "  'question',\n",
       "  'and',\n",
       "  'use',\n",
       "  'contract'],\n",
       " [',',\n",
       "  'Spanish',\n",
       "  ',',\n",
       "  'and',\n",
       "  'French',\n",
       "  '.',\n",
       "  'Do',\n",
       "  \"n't\",\n",
       "  'hesitate',\n",
       "  'to',\n",
       "  'ask',\n",
       "  'question',\n",
       "  'and',\n",
       "  'use',\n",
       "  'contraction'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets compare stemming and lemmatisation\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def stem_text(tokens: list) -> list:\n",
    "    \"\"\" A function that stems text. \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatize_text(tokens: list) -> list:\n",
    "    \"\"\" A function that lemmatizes text. \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "stem_text(tokenize_text(text))[30:45], lemmatize_text(tokenize_text(text))[30:45]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sure',\n",
       "  'to',\n",
       "  'handl',\n",
       "  'acronym',\n",
       "  'like',\n",
       "  'usa',\n",
       "  'and',\n",
       "  'nasa',\n",
       "  'properli',\n",
       "  '.'],\n",
       " ['sure',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'acronym',\n",
       "  'like',\n",
       "  'USA',\n",
       "  'and',\n",
       "  'NASA',\n",
       "  'properly',\n",
       "  '.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text(tokenize_text(text))[-10:], lemmatize_text(tokenize_text(text))[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the differences between the two techniques. Stemming is a crude method that simply chops off the end of a word using heuristics, without taking into account the context of the word. Lemmatisation, on the other hand, uses a more informed analysis to create groups of words with similar meaning based on the context around the word.\n",
    "\n",
    "But lemmatisation can get things wrong without context, for example, the word \"better\" can be a comparative adjective or an adverb, and the lemma of \"better\" is \"good\". So, if we lemmatize the word \"better\" without context, we will get \"good\" as the lemma, which is not correct. This is were Part-of-speech tagging comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech Tagging\n",
    "\n",
    "Part-of-speech tagging is the process of assigning a part-of-speech tag to each word in a given text (corpus). A part-of-speech tag is a special label assigned to each token (word) in a text corpus to indicate the part of speech and often also other grammatical categories such as tense, number (plural/singular), case etc. Part-of-speech tagging is also known as grammatical tagging or word-category disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', ','),\n",
       " ('Spanish', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('French', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('hesitate', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('ask', 'VB'),\n",
       " ('questions', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('use', 'VBP'),\n",
       " ('contractions', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('ca', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('wo', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('.', '.'),\n",
       " ('By', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do pos tagging and then lemmatisation\n",
    "from nltk import pos_tag\n",
    "\n",
    "def pos_tag_text(tokens: list) -> list:\n",
    "    \"\"\" A function that tags text with parts of speech. \"\"\"\n",
    "    return pos_tag(tokens)\n",
    "\n",
    "tagged_tokens = pos_tag_text(tokenize_text(text))\n",
    "tagged_tokens[30:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a table with the different tags meanings [here](https://www.guru99.com/pos-tagging-chunking-nltk.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'VB'\n"
     ]
    }
   ],
   "source": [
    "# now that they are tagged we can use the tags to lemmatise\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatize_tagged_text(tagged_tokens: list) -> list:\n",
    "    \"\"\" A function that lemmatizes text based on its parts of speech. \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token, tag) if tag != None else token for token, tag in tagged_tokens ]\n",
    "\n",
    "try:\n",
    "  lemmatize_tagged_text(tagged_tokens)[30:45]\n",
    "except Exception as e:\n",
    "  # this is not the best way, but to avoid changing the workflow, lets keep it\n",
    "  print(f\"KeyError: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', None),\n",
       " ('Spanish', 'n'),\n",
       " (',', None),\n",
       " ('and', None),\n",
       " ('French', 'n'),\n",
       " ('.', None),\n",
       " ('Do', 'v'),\n",
       " (\"n't\", 'r'),\n",
       " ('hesitate', 'v'),\n",
       " ('to', None),\n",
       " ('ask', 'v'),\n",
       " ('questions', 'n'),\n",
       " ('and', None),\n",
       " ('use', 'v'),\n",
       " ('contractions', 'n'),\n",
       " ('like', None),\n",
       " ('ca', None),\n",
       " (\"n't\", 'r'),\n",
       " ('and', None),\n",
       " ('wo', None),\n",
       " (\"n't\", 'r'),\n",
       " ('.', None),\n",
       " ('By', None),\n",
       " ('the', None),\n",
       " ('way', 'n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "wordnet_compt_tokens = [(tok, get_wordnet_pos(tag))for tok, tag in tagged_tokens]\n",
    "wordnet_compt_tokens[30:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'Spanish',\n",
       " ',',\n",
       " 'and',\n",
       " 'French',\n",
       " '.',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'question',\n",
       " 'and',\n",
       " 'use',\n",
       " 'contraction']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_tagged_text(wordnet_compt_tokens)[30:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another library to do Pos tagging is Spacy \n",
    "Spacy is a library for NLP that has a lot of the features we have seen so far, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI OpenAI PROPN NNP pobj XxxxXX True False\n",
      ". . PUNCT . punct . False False\n",
      "This this PRON DT nsubj Xxxx True True\n",
      "is be AUX VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "demo demo NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "model model NOUN NN poss xxxx True False\n",
      "'s 's PART POS case 'x False True\n",
      "capabilities capability NOUN NNS pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "\n",
      " \n",
      " SPACE _SP dep \n",
      " False False\n",
      "It it PRON PRP nsubj Xx True True\n",
      "can can AUX MD aux xxx True True\n",
      "understand understand VERB VB ROOT xxxx True False\n",
      "various various ADJ JJ amod xxxx True True\n",
      "languages language NOUN NNS dobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "including include VERB VBG prep xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc[10:30]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![meme.jpg](https://i.pinimg.com/originals/37/80/13/378013d1a807149d118b0b254acdf218.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise classify a document\n",
    "\n",
    "Now that we have seen some of the basic NLP preprocessing techniques, let's try to classify some text.\n",
    "\n",
    "We downloaded this [dataset](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download) from Kaggle. It contains a set of tweets labeled with the emotion they express.\n",
    "\n",
    "## Bag of words üõçÔ∏è\n",
    "\n",
    "We will start with a simple (yet powerful) technique called Bag of Words. This technique consists of counting the number of times a word appears in a document. We can use this technique to convert a document into a vector of numbers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/tweet_emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first step would be to assign a numeric value to each sentiment\n",
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'empty',\n",
       " 1: 'sadness',\n",
       " 2: 'enthusiasm',\n",
       " 3: 'neutral',\n",
       " 4: 'worry',\n",
       " 5: 'surprise',\n",
       " 6: 'love',\n",
       " 7: 'fun',\n",
       " 8: 'hate',\n",
       " 9: 'happiness',\n",
       " 10: 'boredom',\n",
       " 11: 'relief',\n",
       " 12: 'anger'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we enumerate the unique values and then convert it to a dictionary where the key is the index and the value is the sentiment\n",
    "id2sent = dict(enumerate(df['sentiment'].unique()))\n",
    "sent2id = {v:k for k,v in id2sent.items()}\n",
    "id2sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "   sentiment_id  \n",
       "0             0  \n",
       "1             1  \n",
       "2             1  \n",
       "3             2  \n",
       "4             3  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can create a column with the numeric values\n",
    "df['sentiment_id'] = df['sentiment'].map(sent2id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sad that the Statue of Liberty will completely reopen 2 weeks after our NYC trip.\n",
      "worry\n",
      "----------------------\n",
      "@dhughesy safer to say that your cube is now a sphere ... but roll with it buddy ... it is an awesome time ... esp the 1st one\n",
      "happiness\n",
      "----------------------\n",
      "@djksly Lol Only if you make me that cookie.  I'll hit you up.\n",
      "neutral\n",
      "----------------------\n",
      "bf moves to the city tomorrow. Currently he lives down the st.  Sadness.  He will be far away  (not TOO far, but not down the street )\n",
      "worry\n",
      "----------------------\n",
      "I couldn't do anythin cuz he jacked it when I stepped out for like 30 seconds &amp; was already outside up the mtn when I came back..\n",
      "worry\n",
      "----------------------\n",
      "will deplurk too. buhbyeee  http://plurk.com/p/rp3ir\n",
      "happiness\n",
      "----------------------\n",
      "Is looking forward to a yummy dinner with mizz Kate jones this evening!\n",
      "fun\n",
      "----------------------\n",
      "FJGKFLD;'Sdh WHY AM I NOT AT HOMETOWN DAYS WITH MY FRIENDS.\n",
      "sadness\n",
      "----------------------\n",
      "OK. break over. Back to the books. Have fun lovelies!  ? http://blip.fm/~5z8da\n",
      "love\n",
      "----------------------\n",
      "@aliceqfoodie Wow, you mom has a lot of energy. I'm getting tired just reading your tweet and ur living it.\n",
      "sadness\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets apply what we learnt so far to preprocess the text\n",
    "# the first step would be to explore the different types of text we have\n",
    "# lets get a random sample and see what we have\n",
    "for _, row in df.sample(10).iterrows():\n",
    "    print(row['content'])\n",
    "    print(row['sentiment'])\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hello world '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see there are usernames that we dont need, a mix of lower and uppercase, some special characters (parenthesis, exclamations and question marks, etc), triple dots, abbreviations, etc\n",
    "# lets start by removing the usernames\n",
    "import re\n",
    "def remove_usernames(text: str) -> str:\n",
    "    \"\"\" A function that removes usernames from text. \"\"\"\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "remove_usernames('@user hello world @user') # test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world this is a bad str ng 2023 is the year'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets remove special characters, triple dots, double spaces, punctuation, etc\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\" A function that gets rid of punctuation, \n",
    "    special characters and extra spaces. \n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.lower() # turn everything to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
    "\n",
    "    return text\n",
    "\n",
    "preprocess_text('HeLlo.World! #this is;, (a bad str!ng)...    2023 is the year') # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>original_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was listenin to bad habit earlier an...</td>\n",
       "      <td>0</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "      <td>1</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>2</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>3</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty   i know i was listenin to bad habit earlier an...   \n",
       "1  1956967666     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
       "2  1956967696     sadness                    funeral ceremony gloomy friday    \n",
       "3  1956967789  enthusiasm               wants to hang out with friends soon    \n",
       "4  1956968416     neutral   we want to trade with someone who has houston...   \n",
       "\n",
       "   sentiment_id                                   original_content  \n",
       "0             0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1             1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2             1                Funeral ceremony...gloomy friday...  \n",
       "3             2               wants to hang out with friends SOON!  \n",
       "4             3  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with what we have learnt so far we cant handle abbreviations, we won't cover that here\n",
    "# but if interested you can search for grammar correctors, using an exhaustive dictionary, or even a probabilistic model\n",
    "\n",
    "df['original_content'] = df['content']\n",
    "# lets put everything together and apply it to the dataframe\n",
    "df['content'] = df['content'].apply(remove_usernames).apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'be', 'run', 'and', 'eat', 'while', 'i', 'write', 'a', 'poem']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have preprocessed the text we can tokenize it\n",
    "# we can use a simple tokenizer from nltk and then lemmatize the tokens, so we can reduce the number of tokens \n",
    "# words like 'running' and 'run' will be converted to 'run' and grouped together\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\" A function that tokenizes text into words. \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tagged_tokens = pos_tag(tokens)\n",
    "    wordnet_compt_tokens = [(tok, get_wordnet_pos(tag))for tok, tag in pos_tagged_tokens]\n",
    "    lemmatized_tokens = lemmatize_tagged_text(wordnet_compt_tokens)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "tokenize_text('i am running and eating while i write a poem') # test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>original_content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was listenin to bad habit earlier an...</td>\n",
       "      <td>0</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>[i, know, i, be, listenin, to, bad, habit, ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "      <td>1</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>2</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>[want, to, hang, out, with, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>3</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, have...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty   i know i was listenin to bad habit earlier an...   \n",
       "1  1956967666     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
       "2  1956967696     sadness                    funeral ceremony gloomy friday    \n",
       "3  1956967789  enthusiasm               wants to hang out with friends soon    \n",
       "4  1956968416     neutral   we want to trade with someone who has houston...   \n",
       "\n",
       "   sentiment_id                                   original_content  \\\n",
       "0             0  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1             1  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2             1                Funeral ceremony...gloomy friday...   \n",
       "3             2               wants to hang out with friends SOON!   \n",
       "4             3  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [i, know, i, be, listenin, to, bad, habit, ear...  \n",
       "1  [layin, n, bed, with, a, headache, ughhhh, wai...  \n",
       "2                [funeral, ceremony, gloomy, friday]  \n",
       "3          [want, to, hang, out, with, friend, soon]  \n",
       "4  [we, want, to, trade, with, someone, who, have...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets apply the tokenizer to the dataframe\n",
    "df['tokens'] = df['content'].apply(tokenize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is an extra preprocessing technique we can use, that we haven't seen yet, and that is stop words removal\n",
    "# Stop words are words that are very common in a language and don't add much meaning to a sentence\n",
    "# Examples of stop words are 'the', 'a', 'an', 'and', 'or', etc\n",
    "# We can use the stop words from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'eat', 'write', 'poem']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do this after tokenizing the text, because the pos tagging works better with these words as context, it can tell if the word is a verb or a noun, etc\n",
    "def remove_stop_words(tokens: list) -> list:\n",
    "    \"\"\" A function that removes stop words from tokens. \"\"\"\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "remove_stop_words(tokenize_text('i am running and eating while i write a poem')) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_id</th>\n",
       "      <th>original_content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was listenin to bad habit earlier an...</td>\n",
       "      <td>0</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>[know, listenin, bad, habit, earlier, start, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "      <td>1</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>[layin, n, bed, headache, ughhhh, waitin, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>2</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>[want, hang, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>3</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>[want, trade, someone, houston, ticket, one]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty   i know i was listenin to bad habit earlier an...   \n",
       "1  1956967666     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
       "2  1956967696     sadness                    funeral ceremony gloomy friday    \n",
       "3  1956967789  enthusiasm               wants to hang out with friends soon    \n",
       "4  1956968416     neutral   we want to trade with someone who has houston...   \n",
       "\n",
       "   sentiment_id                                   original_content  \\\n",
       "0             0  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1             1  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2             1                Funeral ceremony...gloomy friday...   \n",
       "3             2               wants to hang out with friends SOON!   \n",
       "4             3  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [know, listenin, bad, habit, earlier, start, f...  \n",
       "1    [layin, n, bed, headache, ughhhh, waitin, call]  \n",
       "2                [funeral, ceremony, gloomy, friday]  \n",
       "3                         [want, hang, friend, soon]  \n",
       "4       [want, trade, someone, houston, ticket, one]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets apply the stop words removal to the dataframe\n",
    "df['tokens'] = df['tokens'].apply(remove_stop_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'be', 'run', 'and', 'eat', 'while', 'I', 'write', 'a', 'poem'],\n",
       " ['I',\n",
       "  'be',\n",
       "  'a',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'who',\n",
       "  'like',\n",
       "  'to',\n",
       "  'run',\n",
       "  'in',\n",
       "  'the',\n",
       "  'park',\n",
       "  'with',\n",
       "  'my',\n",
       "  'dog'],\n",
       " ['I', 'enjoy', 'watch', 'movie', 'with', 'my', 'friend']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have preprocessed the text we can start building our model\n",
    "# we will use a simple bag of words model, where we will count the number of times a word appears in a sentence\n",
    "\n",
    "\n",
    "# lets do it manually first\n",
    "# we will create a dictionary where the key is the word and the value is the number of times it appears in the sentence\n",
    "# we will use the first sentence as an example\n",
    "# lets suppose we have a corpus of 3 sentences\n",
    "corpus = ['I am running and eating while I write a poem', 'I am a data scientist who likes to run in the park with my dog', 'I enjoy watching movies with my friends']\n",
    "\n",
    "tokens_lst = [tokenize_text(sentence) for sentence in corpus]\n",
    "tokens_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'I': 1,\n",
       " 'a': 2,\n",
       " 'enjoy': 3,\n",
       " 'friend': 4,\n",
       " 'watch': 5,\n",
       " 'park': 6,\n",
       " 'who': 7,\n",
       " 'run': 8,\n",
       " 'movie': 9,\n",
       " 'to': 10,\n",
       " 'in': 11,\n",
       " 'scientist': 12,\n",
       " 'the': 13,\n",
       " 'poem': 14,\n",
       " 'like': 15,\n",
       " 'dog': 16,\n",
       " 'my': 17,\n",
       " 'eat': 18,\n",
       " 'data': 19,\n",
       " 'with': 20,\n",
       " 'write': 21,\n",
       " 'while': 22,\n",
       " 'be': 23}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a dictionary where the key is the word and the value is the number of times it appears in the sentence\n",
    "# the vocabulary will be the set of unique words in the corpus\n",
    "vocab ={token: idx for idx, token in enumerate(set([token for tokens in tokens_lst for token in tokens]))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1],\n",
       " [0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1],\n",
       " [0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a vector for each sentence, where the length of the vector is the length of the vocabulary\n",
    "# the value of each element in the vector will be the number of times the word appears in the sentence\n",
    "bow = [[token.count(word) for word in vocab] for token in tokens_lst]\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0,\n",
       " 'running': 14,\n",
       " 'and': 1,\n",
       " 'eating': 4,\n",
       " 'while': 19,\n",
       " 'write': 22,\n",
       " 'poem': 12,\n",
       " 'data': 2,\n",
       " 'scientist': 15,\n",
       " 'who': 20,\n",
       " 'likes': 8,\n",
       " 'to': 17,\n",
       " 'run': 13,\n",
       " 'in': 7,\n",
       " 'the': 16,\n",
       " 'park': 11,\n",
       " 'with': 21,\n",
       " 'my': 10,\n",
       " 'dog': 3,\n",
       " 'enjoy': 5,\n",
       " 'watching': 18,\n",
       " 'movies': 9,\n",
       " 'friends': 6}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see the vocab takes all the unique words in the corpus and assigns a number to each one, and then we go through \n",
    "# each sentence and count the number of times each word appears in the sentence\n",
    "\n",
    "# this is a very time consuming process, fortunately, we have libraries that can do this for us\n",
    "# we will use the CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# we will use the same corpus as before\n",
    "\n",
    "# we will create an instance of the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# we will fit the vectorizer to the corpus\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# we will get the vocabulary\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will transform the corpus into a bag of words\n",
    "bow = vectorizer.transform(corpus)\n",
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28123"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see the \"I\" was removed. The count vectorizer implementation removes stop words by default, and does tokenization\n",
    "# We already have our text preprocessed, so we will turn off the tokenization and stop words removal\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x,lowercase=False, token_pattern=None)\n",
    "\n",
    "\n",
    "\n",
    "# we will fit the vectorizer to the corpus\n",
    "# but now the corpus is the list of tokens in the dataframe\n",
    "vectorizer.fit(df['tokens'].tolist())\n",
    "\n",
    "\n",
    "# we will get the vocabulary size\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 28123), 9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will transform the corpus into a bag of words\n",
    "# important, we need to wrap the tokens in a list, this is because we turned off the preprocessing and tokenization \n",
    "X = vectorizer.transform(df['tokens'].tolist())\n",
    "X.shape, X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x28123 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 8 stored elements in Compressed Sparse Row format>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are still missing some feature scaling, this will help our model perform better and converge faster\n",
    "# feature scaling is a technique that transforms the data so that it has a mean of 0 and a standard deviation of 1\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X)\n",
    "bow_matrix_scaled = scaler.transform(X)\n",
    "#time with copy = false 56.7s\n",
    "bow_matrix_scaled[:1], bow_matrix_scaled[:1].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.2 ms, sys: 74.8 ms, total: 105 ms\n",
      "Wall time: 4.25 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ernesto/miniforge3/envs/hackathon/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.328875"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# now that we have the bag of words we can start building our model\n",
    "# we will use a simple logistic regression model for multiclass\n",
    "# we will use the bow as the input and the sentiment_id as the target\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# we will split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_matrix_scaled, df['sentiment_id'].tolist(), test_size=0.1)\n",
    "\n",
    "# we will create an instance of the model\n",
    "bow_model = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "# we will fit the model to the training data\n",
    "bow_model.fit(X_train, y_train)\n",
    "\n",
    "# we will get the predictions\n",
    "y_pred = bow_model.predict(X_test)\n",
    "\n",
    "# we will get the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2sent[bow_model.predict(vectorizer.transform(['i am happy'])).tolist()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a pretty low accuracy, this is expected, right now we are only counting the number of times a word appears in a document, we are not scaling them by importance.\n",
    "\n",
    "Let's explore a slight improvement of this technique, TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF ü•Åü•Å\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
    "\n",
    "Basically, we take the bag of words representation and we scale the words by how important they are in the document. This factor is the Inverse Document Frequency (IDF).\n",
    "\n",
    "Let's say the word \"day\" appears 10 times in document 1, but it does not appear in any other document. This means that the word is very important for document 1, and we should make it larger, or make the more common words smaller.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28123"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets compute the tf idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# we will create an instance of the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x,lowercase=False, token_pattern=None)\n",
    "\n",
    "# we will fit the vectorizer to the corpus\n",
    "# but now the corpus is the list of tokens in the dataframe\n",
    "vectorizer.fit(df['tokens'].tolist())\n",
    "\n",
    "# we will get the vocabulary size\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 28123)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.transform(df['tokens'].tolist())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x28123 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 8 stored elements in Compressed Sparse Row format>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1], X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 ms, sys: 9.99 ms, total: 24 ms\n",
      "Wall time: 8.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.343625"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# we will try the same model with the tf idf matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# for demo purposes we won't use the whole dataset\n",
    "df\n",
    "\n",
    "# we will split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment_id'].tolist(), test_size=0.1)\n",
    "\n",
    "# we will create an instance of the model\n",
    "tfidf_model = LogisticRegression(n_jobs=-1, max_iter=1000)\n",
    "\n",
    "# we will fit the model to the training data\n",
    "tfidf_model.fit(X_train, y_train)\n",
    "\n",
    "# we will get the predictions\n",
    "y_pred = tfidf_model.predict(X_test)\n",
    "\n",
    "# we will get the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2sent[tfidf_model.predict(vectorizer.transform(['i am happy'])).tolist()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we want to do, is to explore ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams ü´Ç\n",
    "N-grams are a technique that allows us to capture the context of a word by looking at the words that surround it. For example, the word \"park\" can have a different meaning depending on the words that surround it.\n",
    "\n",
    "- \"I had a great at the park\"\n",
    "- \"I park my car in the garage\"\n",
    "\n",
    "In the first sentence, the word \"park\" is used as a noun, while in the second sentence it is used as a verb.\n",
    "\n",
    "We can use n-grams to capture this context. For example, we can use bigrams (n=2) to capture the context of a word by looking at the words that surround it.\n",
    "\n",
    "- \"I had a great at the park\" -> \"I had\", \"had a\", \"a great\", \"great at\", \"at the\", \"the park\"\n",
    "- \"I park my car in the garage\" -> \"I park\", \"park my\", \"my car\", \"car in\", \"in the\", \"the garage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'am'),\n",
       " ('am', 'running'),\n",
       " ('running', 'and'),\n",
       " ('and', 'eating'),\n",
       " ('eating', 'while'),\n",
       " ('while', 'i'),\n",
       " ('i', 'write'),\n",
       " ('write', 'a'),\n",
       " ('a', 'poem')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are several public implementations of ngrams, we will use the one from nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "def get_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\" A function that returns ngrams from a list of tokens. \"\"\"\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "get_ngrams(['i', 'am', 'running', 'and', 'eating', 'while', 'i', 'write', 'a', 'poem'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383660"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tfidf vectorizer from sklearn has an option to use ngrams.... and do the preprocessing for us too :)\n",
    "\n",
    "# we will create an instance of the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='ascii', lowercase=True, stop_words='english', ngram_range=(1,3), preprocessor=lambda x: remove_usernames(x))\n",
    "\n",
    "# we will fit the vectorizer to the corpus\n",
    "# but now the corpus is the raw text in the dataframe\n",
    "vectorizer.fit(df['original_content'].tolist())\n",
    "\n",
    "# we will get the vocabulary size\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i know i was listenin to bad habit earlier and i started freakin at his part '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.build_preprocessor()(df['original_content'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know',\n",
       " 'was',\n",
       " 'listenin',\n",
       " 'to',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'earlier',\n",
       " 'and',\n",
       " 'started',\n",
       " 'freakin',\n",
       " 'at',\n",
       " 'his',\n",
       " 'part']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.build_tokenizer()(vectorizer.build_preprocessor()(df['original_content'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 383660)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the tfidf matrix\n",
    "X = vectorizer.transform(df['original_content'].tolist())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x383660 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 18 stored elements in Compressed Sparse Row format>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1], X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:8\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we will try the same model with the tf idf matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# we will split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment_id'].tolist(), test_size=0.1)\n",
    "\n",
    "# we will create an instance of the model\n",
    "ngram_model = LogisticRegression(n_jobs=-1, max_iter=1000)\n",
    "\n",
    "# we will fit the model to the training data\n",
    "ngram_model.fit(X_train, y_train)\n",
    "\n",
    "# we will get the predictions\n",
    "y_pred = ngram_model.predict(X_test)\n",
    "\n",
    "# we will get the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2sent[ngram_model.predict(vectorizer.transform(['i am pretty'])).tolist()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "We are using an accuracy metric because it is easy to interpret and to compute, but we did not check if it is appropriate for this dataset. We should check the distribution of the labels to see if it is balanced or not. If it is not balanced, we should use another metric like F1 score.\n",
    "\n",
    "We should have done a deeper EDA to see if there are any patterns in the data that we can use to improve our model.\n",
    "\n",
    "Even though these methods are very powerful, they don't capture the semantics of the text. For example, the sentence \"I love this movie\" and \"I hate this movie\" will have pretty similar representation, even though they have opposite meanings. To capture the semantics of the text we need to use word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
